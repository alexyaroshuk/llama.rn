{
    "sourceFile": "ios/RNLlamaContext.mm",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1733070571466,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1733070571466,
            "name": "Commit-0",
            "content": "#import \"RNLlamaContext.h\"\r\n#import <Metal/Metal.h>\r\n\r\n@implementation RNLlamaContext\r\n\r\n+ (NSDictionary *)modelInfo:(NSString *)path skip:(NSArray *)skip {\r\n    struct lm_gguf_init_params params = {\r\n        /*.no_alloc = */ false,\r\n        /*.ctx      = */ NULL,\r\n    };\r\n\r\n    struct lm_gguf_context * ctx = lm_gguf_init_from_file([path UTF8String], params);\r\n\r\n    if (!ctx) {\r\n        NSLog(@\"%s: failed to load '%s'\\n\", __func__, [path UTF8String]);\r\n        return @{};\r\n    }\r\n\r\n    NSMutableDictionary *info = [[NSMutableDictionary alloc] init];\r\n\r\n    info[@\"version\"] = @(lm_gguf_get_version(ctx));\r\n    info[@\"alignment\"] = @(lm_gguf_get_alignment(ctx));\r\n    info[@\"data_offset\"] = @(lm_gguf_get_data_offset(ctx));\r\n\r\n    // kv\r\n    {\r\n        const int n_kv = lm_gguf_get_n_kv(ctx);\r\n\r\n        for (int i = 0; i < n_kv; ++i) {\r\n            const char * key = lm_gguf_get_key(ctx, i);\r\n\r\n            if (skip && [skip containsObject:[NSString stringWithUTF8String:key]]) {\r\n                continue;\r\n            }\r\n            const std::string value = rnllama::lm_gguf_kv_to_str(ctx, i);\r\n            info[[NSString stringWithUTF8String:key]] = [NSString stringWithUTF8String:value.c_str()];\r\n        }\r\n    }\r\n\r\n    lm_gguf_free(ctx);\r\n\r\n    return info;\r\n}\r\n\r\n+ (instancetype)initWithParams:(NSDictionary *)params onProgress:(void (^)(unsigned int progress))onProgress {\r\n    // llama_backend_init(false);\r\n    common_params defaultParams;\r\n\r\n    if (params[@\"vocab_only\"]) {\r\n        defaultParams.vocab_only = [params[@\"vocab_only\"] boolValue];\r\n        defaultParams.warmup = false;\r\n    }\r\n\r\n    NSString *modelPath = params[@\"model\"];\r\n    BOOL isAsset = [params[@\"is_model_asset\"] boolValue];\r\n    NSString *path = modelPath;\r\n    if (isAsset) path = [[NSBundle mainBundle] pathForResource:modelPath ofType:nil];\r\n\r\n    if (![[NSFileManager defaultManager] fileExistsAtPath:path]) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\"\r\n                                     reason:[NSString stringWithFormat:@\"Model file not found at path: %@\", path]\r\n                                   userInfo:nil];\r\n    }\r\n\r\n    defaultParams.model = [path UTF8String];\r\n\r\n    if (params[@\"n_ctx\"]) defaultParams.n_ctx = [params[@\"n_ctx\"] intValue];\r\n    if (params[@\"use_mlock\"]) defaultParams.use_mlock = [params[@\"use_mlock\"]boolValue];\r\n\r\n    BOOL isMetalEnabled = false;\r\n    NSString *reasonNoMetal = @\"\";\r\n    defaultParams.n_gpu_layers = 0;\r\n    if (params[@\"n_gpu_layers\"] && [params[@\"n_gpu_layers\"] intValue] > 0) {\r\n#ifdef LM_GGML_USE_METAL\r\n        // Check ggml-metal availability\r\n        NSError * error = nil;\r\n        id<MTLDevice> device = MTLCreateSystemDefaultDevice();\r\n        id<MTLLibrary> library = [device\r\n            newLibraryWithSource:@\"#include <metal_stdlib>\\n\"\r\n                                    \"using namespace metal;\"\r\n                                    \"kernel void test() { simd_sum(0); }\"\r\n            options:nil\r\n            error:&error\r\n        ];\r\n        if (error) {\r\n            reasonNoMetal = [error localizedDescription];\r\n        } else {\r\n            id<MTLFunction> kernel = [library newFunctionWithName:@\"test\"];\r\n            id<MTLComputePipelineState> pipeline = [device newComputePipelineStateWithFunction:kernel error:&error];\r\n            if (pipeline == nil) {\r\n                reasonNoMetal = [error localizedDescription];\r\n            } else {\r\n                defaultParams.n_gpu_layers = [params[@\"n_gpu_layers\"] intValue];\r\n                isMetalEnabled = true;\r\n            }\r\n        }\r\n        device = nil;\r\n#else\r\n        reasonNoMetal = @\"Metal is not enabled in this build\";\r\n        isMetalEnabled = false;\r\n#endif\r\n    }\r\n    if (params[@\"n_batch\"]) defaultParams.n_batch = [params[@\"n_batch\"] intValue];\r\n    if (params[@\"use_mmap\"]) defaultParams.use_mmap = [params[@\"use_mmap\"] boolValue];\r\n\r\n    if (params[@\"pooling_type\"] && [params[@\"pooling_type\"] isKindOfClass:[NSNumber class]]) {\r\n      defaultParams.pooling_type = static_cast<enum llama_pooling_type>([params[@\"pooling_type\"] intValue]);\r\n    }\r\n\r\n    if (params[@\"embedding\"] && [params[@\"embedding\"] boolValue]) {\r\n        defaultParams.embedding = true;\r\n        // For non-causal models, batch size must be equal to ubatch size\r\n        defaultParams.n_ubatch = defaultParams.n_batch;\r\n\r\n        if (params[@\"embd_normalize\"] && [params[@\"embd_normalize\"] isKindOfClass:[NSNumber class]]) {\r\n            defaultParams.embd_normalize = [params[@\"embd_normalize\"] intValue];\r\n        }\r\n    }\r\n\r\n    if (params[@\"rope_freq_base\"]) defaultParams.rope_freq_base = [params[@\"rope_freq_base\"] floatValue];\r\n    if (params[@\"rope_freq_scale\"]) defaultParams.rope_freq_scale = [params[@\"rope_freq_scale\"] floatValue];\r\n\r\n    if (params[@\"flash_attn\"] && [params[@\"flash_attn\"] boolValue]) defaultParams.flash_attn = true;\r\n\r\n    if (params[@\"cache_type_k\"]) defaultParams.cache_type_k = [params[@\"cache_type_k\"] UTF8String];\r\n    if (params[@\"cache_type_v\"]) defaultParams.cache_type_v = [params[@\"cache_type_v\"] UTF8String];\r\n\r\n    int nThreads = params[@\"n_threads\"] ? [params[@\"n_threads\"] intValue] : 0;\r\n    const int maxThreads = (int) [[NSProcessInfo processInfo] processorCount];\r\n    // Use 2 threads by default on 4-core devices, 4 threads on more cores\r\n    const int defaultNThreads = nThreads == 4 ? 2 : MIN(4, maxThreads);\r\n    defaultParams.cpuparams.n_threads = nThreads > 0 ? nThreads : defaultNThreads;\r\n\r\n\r\n    RNLlamaContext *context = [[RNLlamaContext alloc] init];\r\n    context->llama = new rnllama::llama_rn_context();\r\n    context->llama->is_load_interrupted = false;\r\n    context->llama->loading_progress = 0;\r\n    context->onProgress = onProgress;\r\n\r\n    if (params[@\"use_progress_callback\"] && [params[@\"use_progress_callback\"] boolValue]) {\r\n        defaultParams.progress_callback = [](float progress, void * user_data) {\r\n            RNLlamaContext *context = (__bridge RNLlamaContext *)(user_data);\r\n            unsigned percentage = (unsigned) (100 * progress);\r\n            if (percentage > context->llama->loading_progress) {\r\n                context->llama->loading_progress = percentage;\r\n                context->onProgress(percentage);\r\n            }\r\n            return !context->llama->is_load_interrupted;\r\n        };\r\n        defaultParams.progress_callback_user_data = context;\r\n    }\r\n\r\n    context->is_model_loaded = context->llama->loadModel(defaultParams);\r\n\r\n    if (!context->is_model_loaded) {\r\n        NSString *errorMsg = [NSString stringWithFormat:@\"Failed to load model at path: %@. Please check file permissions and memory availability.\", path];\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:errorMsg userInfo:nil];\r\n    }\r\n\r\n    if (\r\n        params[@\"embedding\"] && [params[@\"embedding\"] boolValue] &&\r\n        llama_model_has_encoder(context->llama->model) && llama_model_has_decoder(context->llama->model)\r\n    ) {\r\n        delete context->llama;\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Embedding is not supported in encoder-decoder models\" userInfo:nil];\r\n    }\r\n\r\n    std::vector<common_lora_adapter_info> lora_adapters;\r\n    if (params[@\"lora\"]) {\r\n        common_lora_adapter_info la;\r\n        la.path = [params[@\"lora\"] UTF8String];\r\n        la.scale = 1.0f;\r\n        if (params[@\"lora_scaled\"]) la.scale = [params[@\"lora_scaled\"] floatValue];\r\n        lora_adapters.push_back(la);\r\n    }\r\n    if (params[@\"lora_list\"] && [params[@\"lora_list\"] isKindOfClass:[NSArray class]]) {\r\n        NSArray *lora_list = params[@\"lora_list\"];\r\n        for (NSDictionary *lora_adapter in lora_list) {\r\n          NSString *path = lora_adapter[@\"path\"];\r\n          if (!path) continue;\r\n          float scale = [lora_adapter[@\"scaled\"] floatValue];\r\n          common_lora_adapter_info la;\r\n          la.path = [path UTF8String];\r\n          la.scale = scale;\r\n          lora_adapters.push_back(la);\r\n        }\r\n    }\r\n    if (lora_adapters.size() > 0) {\r\n        int result = context->llama->applyLoraAdapters(lora_adapters);\r\n        if (result != 0) {\r\n            delete context->llama;\r\n            @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Failed to apply lora adapters\" userInfo:nil];\r\n        }\r\n    }\r\n\r\n    context->is_metal_enabled = isMetalEnabled;\r\n    context->reason_no_metal = reasonNoMetal;\r\n\r\n    return context;\r\n}\r\n\r\n- (void)interruptLoad {\r\n    llama->is_load_interrupted = true;\r\n}\r\n\r\n- (bool)isMetalEnabled {\r\n    return is_metal_enabled;\r\n}\r\n\r\n- (NSString *)reasonNoMetal {\r\n    return reason_no_metal;\r\n}\r\n\r\n- (NSDictionary *)modelInfo {\r\n    char desc[1024];\r\n    llama_model_desc(llama->model, desc, sizeof(desc));\r\n\r\n    int count = llama_model_meta_count(llama->model);\r\n    NSDictionary *meta = [[NSMutableDictionary alloc] init];\r\n    for (int i = 0; i < count; i++) {\r\n        char key[256];\r\n        llama_model_meta_key_by_index(llama->model, i, key, sizeof(key));\r\n        char val[2048];\r\n        llama_model_meta_val_str_by_index(llama->model, i, val, sizeof(val));\r\n\r\n        NSString *keyStr = [NSString stringWithUTF8String:key];\r\n        NSString *valStr = [NSString stringWithUTF8String:val];\r\n        [meta setValue:valStr forKey:keyStr];\r\n    }\r\n\r\n    return @{\r\n        @\"desc\": [NSString stringWithUTF8String:desc],\r\n        @\"size\": @(llama_model_size(llama->model)),\r\n        @\"nParams\": @(llama_model_n_params(llama->model)),\r\n        @\"isChatTemplateSupported\": @(llama->validateModelChatTemplate()),\r\n        @\"metadata\": meta\r\n    };\r\n}\r\n\r\n- (bool)isModelLoaded {\r\n    return is_model_loaded;\r\n}\r\n\r\n- (bool)isPredicting {\r\n    return llama->is_predicting;\r\n}\r\n\r\n- (NSString *)getFormattedChat:(NSArray *)messages withTemplate:(NSString *)chatTemplate {\r\n  std::vector<common_chat_msg> chat;\r\n\r\n  for (NSDictionary *msg in messages) {\r\n    std::string role = [[msg objectForKey:@\"role\"] UTF8String];\r\n    std::string content = [[msg objectForKey:@\"content\"] UTF8String];\r\n    chat.push_back({ role, content });\r\n  }\r\n\r\n  auto tmpl = chatTemplate == nil ? \"\" : [chatTemplate UTF8String];\r\n  auto formatted_chat = common_chat_apply_template(llama->model, tmpl, chat, true);\r\n  return [NSString stringWithUTF8String:formatted_chat.c_str()];\r\n}\r\n\r\n- (NSArray *)tokenProbsToDict:(std::vector<rnllama::completion_token_output>)probs {\r\n    NSMutableArray *out = [[NSMutableArray alloc] init];\r\n    for (const auto &prob : probs)\r\n    {\r\n        NSMutableArray *probsForToken = [[NSMutableArray alloc] init];\r\n        for (const auto &p : prob.probs)\r\n        {\r\n            std::string tokStr = rnllama::tokens_to_output_formatted_string(llama->ctx, p.tok);\r\n            [probsForToken addObject:@{\r\n                @\"tok_str\": [NSString stringWithUTF8String:tokStr.c_str()],\r\n                @\"prob\": [NSNumber numberWithDouble:p.prob]\r\n            }];\r\n        }\r\n        std::string tokStr = rnllama::tokens_to_output_formatted_string(llama->ctx, prob.tok);\r\n        [out addObject:@{\r\n            @\"content\": [NSString stringWithUTF8String:tokStr.c_str()],\r\n            @\"probs\": probsForToken\r\n        }];\r\n    }\r\n    return out;\r\n}\r\n\r\n- (NSDictionary *)completion:(NSDictionary *)params\r\n    onToken:(void (^)(NSMutableDictionary * tokenResult))onToken\r\n{\r\n    llama->rewind();\r\n\r\n    //llama_reset_timings(llama->ctx);\r\n\r\n    NSString *prompt = [params objectForKey:@\"prompt\"];\r\n\r\n    llama->params.prompt = [prompt UTF8String];\r\n    llama->params.sparams.seed = params[@\"seed\"] ? [params[@\"seed\"] intValue] : -1;\r\n\r\n    if (params[@\"n_threads\"]) {\r\n        int nThreads = params[@\"n_threads\"] ? [params[@\"n_threads\"] intValue] : llama->params.cpuparams.n_threads;\r\n        const int maxThreads = (int) [[NSProcessInfo processInfo] processorCount];\r\n        // Use 2 threads by default on 4-core devices, 4 threads on more cores\r\n        const int defaultNThreads = nThreads == 4 ? 2 : MIN(4, maxThreads);\r\n        llama->params.cpuparams.n_threads = nThreads > 0 ? nThreads : defaultNThreads;\r\n    }\r\n    if (params[@\"n_predict\"]) llama->params.n_predict = [params[@\"n_predict\"] intValue];\r\n    if (params[@\"ignore_eos\"]) llama->params.sparams.ignore_eos = [params[@\"ignore_eos\"] boolValue];\r\n\r\n    auto & sparams = llama->params.sparams;\r\n\r\n    if (params[@\"temperature\"]) sparams.temp = [params[@\"temperature\"] doubleValue];\r\n\r\n    if (params[@\"n_probs\"]) sparams.n_probs = [params[@\"n_probs\"] intValue];\r\n\r\n    if (params[@\"penalty_last_n\"]) sparams.penalty_last_n = [params[@\"penalty_last_n\"] intValue];\r\n    if (params[@\"penalty_repeat\"]) sparams.penalty_repeat = [params[@\"penalty_repeat\"] doubleValue];\r\n    if (params[@\"penalty_freq\"]) sparams.penalty_freq = [params[@\"penalty_freq\"] doubleValue];\r\n    if (params[@\"penalty_present\"]) sparams.penalty_present = [params[@\"penalty_present\"] doubleValue];\r\n\r\n    if (params[@\"mirostat\"]) sparams.mirostat = [params[@\"mirostat\"] intValue];\r\n    if (params[@\"mirostat_tau\"]) sparams.mirostat_tau = [params[@\"mirostat_tau\"] doubleValue];\r\n    if (params[@\"mirostat_eta\"]) sparams.mirostat_eta = [params[@\"mirostat_eta\"] doubleValue];\r\n    if (params[@\"penalize_nl\"]) sparams.penalize_nl = [params[@\"penalize_nl\"] boolValue];\r\n\r\n    if (params[@\"top_k\"]) sparams.top_k = [params[@\"top_k\"] intValue];\r\n    if (params[@\"top_p\"]) sparams.top_p = [params[@\"top_p\"] doubleValue];\r\n    if (params[@\"min_p\"]) sparams.min_p = [params[@\"min_p\"] doubleValue];\r\n    if (params[@\"xtc_threshold\"]) sparams.xtc_threshold = [params[@\"xtc_threshold\"] doubleValue];\r\n    if (params[@\"xtc_probability\"]) sparams.xtc_probability = [params[@\"xtc_probability\"] doubleValue];\r\n    if (params[@\"typical_p\"]) sparams.typ_p = [params[@\"typical_p\"] doubleValue];\r\n\r\n    if (params[@\"dry_multiplier\"]) sparams.dry_multiplier = [params[@\"dry_multiplier\"] doubleValue];\r\n    if (params[@\"dry_base\"]) sparams.dry_base = [params[@\"dry_base\"] doubleValue];\r\n    if (params[@\"dry_allowed_length\"]) sparams.dry_allowed_length = [params[@\"dry_allowed_length\"] intValue];\r\n    if (params[@\"dry_penalty_last_n\"]) sparams.dry_penalty_last_n = [params[@\"dry_penalty_last_n\"] intValue];\r\n\r\n    // dry break seq\r\n    if (params[@\"dry_sequence_breakers\"] && [params[@\"dry_sequence_breakers\"] isKindOfClass:[NSArray class]]) {\r\n        NSArray *dry_sequence_breakers = params[@\"dry_sequence_breakers\"];\r\n        for (NSString *s in dry_sequence_breakers) {\r\n            sparams.dry_sequence_breakers.push_back([s UTF8String]);\r\n        }\r\n    }\r\n\r\n    if (params[@\"grammar\"]) {\r\n        sparams.grammar = [params[@\"grammar\"] UTF8String];\r\n    }\r\n\r\n    llama->params.antiprompt.clear();\r\n    if (params[@\"stop\"]) {\r\n        NSArray *stop = params[@\"stop\"];\r\n        for (NSString *s in stop) {\r\n            llama->params.antiprompt.push_back([s UTF8String]);\r\n        }\r\n    }\r\n\r\n    sparams.logit_bias.clear();\r\n    if (params[@\"ignore_eos\"] && [params[@\"ignore_eos\"] boolValue]) {\r\n        sparams.logit_bias[llama_token_eos(llama->model)].bias = -INFINITY;\r\n    }\r\n\r\n    if (params[@\"logit_bias\"] && [params[@\"logit_bias\"] isKindOfClass:[NSArray class]]) {\r\n        const int n_vocab = llama_n_vocab(llama_get_model(llama->ctx));\r\n        NSArray *logit_bias = params[@\"logit_bias\"];\r\n        for (NSArray *el in logit_bias) {\r\n            if ([el isKindOfClass:[NSArray class]] && [el count] == 2) {\r\n                llama_token tok = [el[0] intValue];\r\n                if (tok >= 0 && tok < n_vocab) {\r\n                    if ([el[1] isKindOfClass:[NSNumber class]]) {\r\n                        sparams.logit_bias[tok].bias = [el[1] doubleValue];\r\n                    } else if ([el[1] isKindOfClass:[NSNumber class]] && ![el[1] boolValue]) {\r\n                        sparams.logit_bias[tok].bias = -INFINITY;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    if (!llama->initSampling()) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Failed to initialize sampling\" userInfo:nil];\r\n    }\r\n    llama->beginCompletion();\r\n    llama->loadPrompt();\r\n\r\n    size_t sent_count = 0;\r\n    size_t sent_token_probs_index = 0;\r\n\r\n    while (llama->has_next_token && !llama->is_interrupted) {\r\n        const rnllama::completion_token_output token_with_probs = llama->doCompletion();\r\n        if (token_with_probs.tok == -1 || llama->incomplete) {\r\n            continue;\r\n        }\r\n        const std::string token_text = common_token_to_piece(llama->ctx, token_with_probs.tok);\r\n\r\n        size_t pos = std::min(sent_count, llama->generated_text.size());\r\n\r\n        const std::string str_test = llama->generated_text.substr(pos);\r\n        bool is_stop_full = false;\r\n        size_t stop_pos =\r\n            llama->findStoppingStrings(str_test, token_text.size(), rnllama::STOP_FULL);\r\n        if (stop_pos != std::string::npos) {\r\n            is_stop_full = true;\r\n            llama->generated_text.erase(\r\n                llama->generated_text.begin() + pos + stop_pos,\r\n                llama->generated_text.end());\r\n            pos = std::min(sent_count, llama->generated_text.size());\r\n        } else {\r\n            is_stop_full = false;\r\n            stop_pos = llama->findStoppingStrings(str_test, token_text.size(),\r\n                rnllama::STOP_PARTIAL);\r\n        }\r\n\r\n        if (\r\n            stop_pos == std::string::npos ||\r\n            // Send rest of the text if we are at the end of the generation\r\n            (!llama->has_next_token && !is_stop_full && stop_pos > 0)\r\n        ) {\r\n            const std::string to_send = llama->generated_text.substr(pos, std::string::npos);\r\n\r\n            sent_count += to_send.size();\r\n\r\n            std::vector<rnllama::completion_token_output> probs_output = {};\r\n\r\n            NSMutableDictionary *tokenResult = [[NSMutableDictionary alloc] init];\r\n            tokenResult[@\"token\"] = [NSString stringWithUTF8String:to_send.c_str()];\r\n\r\n            if (llama->params.sparams.n_probs > 0) {\r\n                const std::vector<llama_token> to_send_toks = common_tokenize(llama->ctx, to_send, false);\r\n                size_t probs_pos = std::min(sent_token_probs_index, llama->generated_token_probs.size());\r\n                size_t probs_stop_pos = std::min(sent_token_probs_index + to_send_toks.size(), llama->generated_token_probs.size());\r\n                if (probs_pos < probs_stop_pos) {\r\n                    probs_output = std::vector<rnllama::completion_token_output>(llama->generated_token_probs.begin() + probs_pos, llama->generated_token_probs.begin() + probs_stop_pos);\r\n                }\r\n                sent_token_probs_index = probs_stop_pos;\r\n\r\n                tokenResult[@\"completion_probabilities\"] = [self tokenProbsToDict:probs_output];\r\n            }\r\n\r\n            onToken(tokenResult);\r\n        }\r\n    }\r\n\r\n    llama_perf_context_print(llama->ctx);\r\n    llama->is_predicting = false;\r\n\r\n    const auto timings = llama_perf_context(llama->ctx);\r\n    return @{\r\n        @\"text\": [NSString stringWithUTF8String:llama->generated_text.c_str()],\r\n        @\"completion_probabilities\": [self tokenProbsToDict:llama->generated_token_probs],\r\n        @\"tokens_predicted\": @(llama->num_tokens_predicted),\r\n        @\"tokens_evaluated\": @(llama->num_prompt_tokens),\r\n        @\"truncated\": @(llama->truncated),\r\n        @\"stopped_eos\": @(llama->stopped_eos),\r\n        @\"stopped_word\": @(llama->stopped_word),\r\n        @\"stopped_limit\": @(llama->stopped_limit),\r\n        @\"stopping_word\": [NSString stringWithUTF8String:llama->stopping_word.c_str()],\r\n        @\"tokens_cached\": @(llama->n_past),\r\n        @\"timings\": @{\r\n            @\"prompt_n\": @(timings.n_p_eval),\r\n            @\"prompt_ms\": @(timings.t_p_eval_ms),\r\n            @\"prompt_per_token_ms\": @(timings.t_p_eval_ms / timings.n_p_eval),\r\n            @\"prompt_per_second\": @(1e3 / timings.t_p_eval_ms * timings.n_p_eval),\r\n\r\n            @\"predicted_n\": @(timings.n_eval),\r\n            @\"predicted_ms\": @(timings.t_eval_ms),\r\n            @\"predicted_per_token_ms\": @(timings.t_eval_ms / timings.n_eval),\r\n            @\"predicted_per_second\": @(1e3 / timings.t_eval_ms * timings.n_eval),\r\n        }\r\n    };\r\n}\r\n\r\n- (void)stopCompletion {\r\n    llama->is_interrupted = true;\r\n}\r\n\r\n- (NSArray *)tokenize:(NSString *)text {\r\n    const std::vector<llama_token> toks = common_tokenize(llama->ctx, [text UTF8String], false);\r\n    NSMutableArray *result = [[NSMutableArray alloc] init];\r\n    for (llama_token tok : toks) {\r\n        [result addObject:@(tok)];\r\n    }\r\n    return result;\r\n}\r\n\r\n- (NSString *)detokenize:(NSArray *)tokens {\r\n    std::vector<llama_token> toks;\r\n    for (NSNumber *tok in tokens) {\r\n        toks.push_back([tok intValue]);\r\n    }\r\n    const std::string text = rnllama::tokens_to_str(llama->ctx, toks.cbegin(), toks.cend());\r\n    return [NSString stringWithUTF8String:text.c_str()];\r\n}\r\n\r\n- (NSDictionary *)embedding:(NSString *)text params:(NSDictionary *)params {\r\n    if (llama->params.embedding != true) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Embedding is not enabled\" userInfo:nil];\r\n    }\r\n\r\n    common_params embdParams;\r\n    embdParams.embedding = true;\r\n    embdParams.embd_normalize = llama->params.embd_normalize;\r\n\r\n    if (params[@\"embd_normalize\"] && [params[@\"embd_normalize\"] isKindOfClass:[NSNumber class]]) {\r\n        embdParams.embd_normalize = [params[@\"embd_normalize\"] intValue];\r\n    }\r\n\r\n    llama->rewind();\r\n\r\n    llama_perf_context_reset(llama->ctx);\r\n\r\n    llama->params.prompt = [text UTF8String];\r\n\r\n    llama->params.n_predict = 0;\r\n\r\n    if (!llama->initSampling()) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Failed to initialize sampling\" userInfo:nil];\r\n    }\r\n    llama->beginCompletion();\r\n    llama->loadPrompt();\r\n    llama->doCompletion();\r\n\r\n    std::vector<float> result = llama->getEmbedding(embdParams);\r\n\r\n    NSMutableDictionary *resultDict = [[NSMutableDictionary alloc] init];\r\n    NSMutableArray *embeddingResult = [[NSMutableArray alloc] init];\r\n    for (float f : result) {\r\n        [embeddingResult addObject:@(f)];\r\n    }\r\n    resultDict[@\"embedding\"] = embeddingResult;\r\n    NSMutableArray *promptTokens = [[NSMutableArray alloc] init];\r\n    for (llama_token tok : llama->embd) {\r\n        [promptTokens addObject:[NSString stringWithUTF8String:common_token_to_piece(llama->ctx, tok).c_str()]];\r\n    }\r\n    resultDict[@\"prompt_tokens\"] = promptTokens;\r\n\r\n    llama->is_predicting = false;\r\n    return resultDict;\r\n}\r\n\r\n- (NSDictionary *)loadSession:(NSString *)path {\r\n    if (!path || [path length] == 0) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Session path is empty\" userInfo:nil];\r\n    }\r\n    if (![[NSFileManager defaultManager] fileExistsAtPath:path]) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Session file does not exist\" userInfo:nil];\r\n    }\r\n\r\n    size_t n_token_count_out = 0;\r\n    llama->embd.resize(llama->params.n_ctx);\r\n    if (!llama_state_load_file(llama->ctx, [path UTF8String], llama->embd.data(), llama->embd.capacity(), &n_token_count_out)) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Failed to load session\" userInfo:nil];\r\n    }\r\n    llama->embd.resize(n_token_count_out);\r\n    const std::string text = rnllama::tokens_to_str(llama->ctx, llama->embd.cbegin(), llama->embd.cend());\r\n    return @{\r\n        @\"tokens_loaded\": @(n_token_count_out),\r\n        @\"prompt\": [NSString stringWithUTF8String:text.c_str()]\r\n    };\r\n}\r\n\r\n- (int)saveSession:(NSString *)path size:(int)size {\r\n    if (!path || [path length] == 0) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Session path is empty\" userInfo:nil];\r\n    }\r\n    std::vector<llama_token> session_tokens = llama->embd;\r\n    int default_size = session_tokens.size();\r\n    int save_size = size > 0 && size <= default_size ? size : default_size;\r\n    if (!llama_state_save_file(llama->ctx, [path UTF8String], session_tokens.data(), save_size)) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Failed to save session\" userInfo:nil];\r\n    }\r\n    return session_tokens.size();\r\n}\r\n\r\n- (NSString *)bench:(int)pp tg:(int)tg pl:(int)pl nr:(int)nr {\r\n    return [NSString stringWithUTF8String:llama->bench(pp, tg, pl, nr).c_str()];\r\n}\r\n\r\n- (void)applyLoraAdapters:(NSArray *)loraAdapters {\r\n    std::vector<common_lora_adapter_info> lora_adapters;\r\n    for (NSDictionary *loraAdapter in loraAdapters) {\r\n        common_lora_adapter_info la;\r\n        la.path = [loraAdapter[@\"path\"] UTF8String];\r\n        la.scale = [loraAdapter[@\"scaled\"] doubleValue];\r\n        lora_adapters.push_back(la);\r\n    }\r\n    int result = llama->applyLoraAdapters(lora_adapters);\r\n    if (result != 0) {\r\n        @throw [NSException exceptionWithName:@\"LlamaException\" reason:@\"Failed to apply lora adapters\" userInfo:nil];\r\n    }\r\n}\r\n\r\n- (void)removeLoraAdapters {\r\n    llama->removeLoraAdapters();\r\n}\r\n\r\n- (NSArray *)getLoadedLoraAdapters {\r\n    std::vector<common_lora_adapter_container> loaded_lora_adapters = llama->getLoadedLoraAdapters();\r\n    NSMutableArray *result = [[NSMutableArray alloc] init];\r\n    for (common_lora_adapter_container &la : loaded_lora_adapters) {\r\n        [result addObject:@{\r\n            @\"path\": [NSString stringWithUTF8String:la.path.c_str()],\r\n            @\"scale\": @(la.scale)\r\n        }];\r\n    }\r\n    return result;\r\n}\r\n\r\n- (void)invalidate {\r\n    delete llama;\r\n    // llama_backend_free();\r\n}\r\n\r\n@end\r\n"
        }
    ]
}
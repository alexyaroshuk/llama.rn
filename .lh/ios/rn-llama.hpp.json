{
    "sourceFile": "ios/rn-llama.hpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 6,
            "patches": [
                {
                    "date": 1733070571467,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733072616214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,83 @@\n+#pragma once\r\n+\r\n+#include <string>\r\n+#include <exception>\r\n+#include <vector>\r\n+#include \"llama.h\"\r\n+#include \"common.h\"\r\n+\r\n+namespace rnllama {\r\n+\r\n+struct completion_token_output {\r\n+    llama_token tok;\r\n+    std::vector<llama_token_data> probs;\r\n+};\r\n+\r\n+class llama_rn_context {\r\n+private:\r\n+    llama_model* model;\r\n+    llama_context* ctx;\r\n+    bool is_load_interrupted;\r\n+    int loading_progress;\r\n+    std::string last_error;\r\n+\r\n+public:\r\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n+\r\n+    ~llama_rn_context() {\r\n+        if (ctx) llama_free(ctx);\r\n+        if (model) llama_free_model(model);\r\n+    }\r\n+\r\n+    bool load_model(const common_params& params, NSString** error) {\r\n+        try {\r\n+            // Check file access\r\n+            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n+            if (!f) {\r\n+                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+            fclose(f);\r\n+\r\n+            // Load model\r\n+            llama_model_params model_params = llama_model_default_params();\r\n+            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n+\r\n+            if (!model) {\r\n+                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+\r\n+            // Initialize context\r\n+            llama_context_params ctx_params = llama_context_default_params();\r\n+            ctx_params.n_ctx = params.n_ctx;\r\n+            ctx = llama_new_context_with_model(model, ctx_params);\r\n+\r\n+            if (!ctx) {\r\n+                last_error = \"Failed to create context\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                llama_free_model(model);\r\n+                model = nullptr;\r\n+                return false;\r\n+            }\r\n+\r\n+            return true;\r\n+\r\n+        } catch (const std::exception& e) {\r\n+            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n+            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+            return false;\r\n+        }\r\n+    }\r\n+\r\n+    llama_model* get_model() const { return model; }\r\n+    llama_context* get_context() const { return ctx; }\r\n+    bool get_is_load_interrupted() const { return is_load_interrupted; }\r\n+    void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\r\n+    int get_loading_progress() const { return loading_progress; }\r\n+    void set_loading_progress(int value) { loading_progress = value; }\r\n+};\r\n+\r\n+} // namespace rnllama\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733073426866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,26 +4,34 @@\n #include <exception>\r\n #include <vector>\r\n #include \"llama.h\"\r\n #include \"common.h\"\r\n+#include \"ggml.h\"\r\n \r\n namespace rnllama {\r\n \r\n+// Helper function to convert GGUF key-value to string\r\n+inline std::string lm_gguf_kv_to_str(struct lm_gguf_context* ctx, int i) {\r\n+    return std::string(lm_gguf_get_val_str(ctx, i));\r\n+}\r\n+\r\n struct completion_token_output {\r\n     llama_token tok;\r\n     std::vector<llama_token_data> probs;\r\n };\r\n \r\n class llama_rn_context {\r\n-private:\r\n+public:  // Make these public since they're accessed from Objective-C\r\n     llama_model* model;\r\n     llama_context* ctx;\r\n     bool is_load_interrupted;\r\n     int loading_progress;\r\n+    bool is_predicting;\r\n     std::string last_error;\r\n \r\n public:\r\n-    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false),\r\n+                        loading_progress(0), is_predicting(false) {}\r\n \r\n     ~llama_rn_context() {\r\n         if (ctx) llama_free(ctx);\r\n         if (model) llama_free_model(model);\r\n@@ -71,8 +79,19 @@\n             return false;\r\n         }\r\n     }\r\n \r\n+    bool validateModelChatTemplate() const {\r\n+        // Add implementation based on your requirements\r\n+        return true;\r\n+    }\r\n+\r\n+    int applyLoraAdapters(const std::vector<common_lora_adapter_info>& adapters) {\r\n+        // Add implementation based on your requirements\r\n+        return 0;\r\n+    }\r\n+\r\n+    // Getter methods for compatibility\r\n     llama_model* get_model() const { return model; }\r\n     llama_context* get_context() const { return ctx; }\r\n     bool get_is_load_interrupted() const { return is_load_interrupted; }\r\n     void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\r\n"
                },
                {
                    "date": 1733074158325,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,25 +13,77 @@\n inline std::string lm_gguf_kv_to_str(struct lm_gguf_context* ctx, int i) {\r\n     return std::string(lm_gguf_get_val_str(ctx, i));\r\n }\r\n \r\n+// Helper function to format tokens to string\r\n+inline std::string tokens_to_output_formatted_string(llama_context* ctx, llama_token token) {\r\n+    const char* str = llama_token_to_str(ctx, token);\r\n+    return std::string(str ? str : \"\");\r\n+}\r\n+\r\n+struct llama_token_data_with_prob {\r\n+    llama_token tok;\r\n+    float prob;\r\n+};\r\n+\r\n struct completion_token_output {\r\n     llama_token tok;\r\n-    std::vector<llama_token_data> probs;\r\n+    std::vector<llama_token_data_with_prob> probs;\r\n };\r\n \r\n+// Sampling parameters structure\r\n+struct sampling_params {\r\n+    int32_t seed;\r\n+    int32_t n_predict;\r\n+    int32_t top_k;\r\n+    float top_p;\r\n+    float temp;\r\n+    float penalty_repeat;\r\n+    int32_t penalty_last_n;\r\n+    bool ignore_eos;\r\n+    std::vector<std::string> antiprompt;\r\n+};\r\n+\r\n+// CPU parameters structure\r\n+struct cpu_params {\r\n+    int32_t n_threads;\r\n+};\r\n+\r\n+// Parameters structure\r\n+struct params {\r\n+    std::string prompt;\r\n+    sampling_params sparams;\r\n+    cpu_params cpuparams;\r\n+    int32_t n_predict;\r\n+};\r\n+\r\n class llama_rn_context {\r\n public:  // Make these public since they're accessed from Objective-C\r\n     llama_model* model;\r\n     llama_context* ctx;\r\n     bool is_load_interrupted;\r\n     int loading_progress;\r\n     bool is_predicting;\r\n+    bool is_interrupted;\r\n+    bool has_next_token;\r\n     std::string last_error;\r\n+    params params;\r\n \r\n public:\r\n     llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false),\r\n-                        loading_progress(0), is_predicting(false) {}\r\n+                        loading_progress(0), is_predicting(false), is_interrupted(false),\r\n+                        has_next_token(false) {\r\n+        // Initialize default parameters\r\n+        params.sparams.seed = -1;\r\n+        params.sparams.n_predict = 128;\r\n+        params.sparams.top_k = 40;\r\n+        params.sparams.top_p = 0.95f;\r\n+        params.sparams.temp = 0.8f;\r\n+        params.sparams.penalty_repeat = 1.1f;\r\n+        params.sparams.penalty_last_n = 64;\r\n+        params.sparams.ignore_eos = false;\r\n+        params.cpuparams.n_threads = 4;\r\n+    }\r\n \r\n     ~llama_rn_context() {\r\n         if (ctx) llama_free(ctx);\r\n         if (model) llama_free_model(model);\r\n@@ -79,15 +131,36 @@\n             return false;\r\n         }\r\n     }\r\n \r\n+    void rewind() {\r\n+        if (ctx) {\r\n+            llama_kv_cache_clear(ctx);\r\n+        }\r\n+    }\r\n+\r\n+    bool initSampling() {\r\n+        if (!ctx || !model) return false;\r\n+        has_next_token = true;\r\n+        is_interrupted = false;\r\n+        return true;\r\n+    }\r\n+\r\n+    void beginCompletion() {\r\n+        is_predicting = true;\r\n+    }\r\n+\r\n+    void loadPrompt() {\r\n+        if (!ctx || params.prompt.empty()) return;\r\n+        std::vector<llama_token> tokens = llama_tokenize(ctx, params.prompt.c_str(), true);\r\n+        llama_eval(ctx, tokens.data(), tokens.size(), 0, params.cpuparams.n_threads);\r\n+    }\r\n+\r\n     bool validateModelChatTemplate() const {\r\n-        // Add implementation based on your requirements\r\n         return true;\r\n     }\r\n \r\n     int applyLoraAdapters(const std::vector<common_lora_adapter_info>& adapters) {\r\n-        // Add implementation based on your requirements\r\n         return 0;\r\n     }\r\n \r\n     // Getter methods for compatibility\r\n@@ -98,61 +171,5 @@\n     int get_loading_progress() const { return loading_progress; }\r\n     void set_loading_progress(int value) { loading_progress = value; }\r\n };\r\n \r\n-} // namespace rnllama\n-class llama_rn_context {\r\n-private:\r\n-    llama_model* model;\r\n-    llama_context* ctx;\r\n-    bool is_load_interrupted;\r\n-    int loading_progress;\r\n-    std::string last_error;\r\n-\r\n-public:\r\n-    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n-\r\n-    bool load_model(const common_params& params, NSString** error) {\r\n-        try {\r\n-            // Check file access\r\n-            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n-            if (!f) {\r\n-                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n-                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-                return false;\r\n-            }\r\n-            fclose(f);\r\n-\r\n-            // Load model\r\n-            llama_model_params model_params = llama_model_default_params();\r\n-            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n-\r\n-            if (!model) {\r\n-                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n-                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-                return false;\r\n-            }\r\n-\r\n-            // Initialize context\r\n-            llama_context_params ctx_params = llama_context_default_params();\r\n-            ctx_params.n_ctx = params.n_ctx;\r\n-            ctx = llama_new_context_with_model(model, ctx_params);\r\n-\r\n-            if (!ctx) {\r\n-                last_error = \"Failed to create context\";\r\n-                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-                llama_free_model(model);\r\n-                model = nullptr;\r\n-                return false;\r\n-            }\r\n-\r\n-            return true;\r\n-\r\n-        } catch (const std::exception& e) {\r\n-            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n-            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-            return false;\r\n-        }\r\n-    }\r\n-\r\n-    // ... rest of the class implementation ...\r\n-};\n\\ No newline at end of file\n+} // namespace rnllama\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733074469608,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,204 @@\n+#pragma once\r\n+\r\n+#include <string>\r\n+#include <exception>\r\n+#include <vector>\r\n+#include \"llama.h\"\r\n+#include \"common.h\"\r\n+#include \"ggml.h\"\r\n+\r\n+namespace rnllama {\r\n+\r\n+// Helper function to convert GGUF key-value to string\r\n+inline std::string lm_gguf_kv_to_str(struct lm_gguf_context* ctx, int i) {\r\n+    return std::string(lm_gguf_get_val_str(ctx, i));\r\n+}\r\n+\r\n+// Helper function to format tokens to string\r\n+inline std::string tokens_to_output_formatted_string(llama_context* ctx, llama_token token) {\r\n+    std::vector<char> result(32, 0);\r\n+    llama_token_to_piece(ctx, token, result.data(), result.size());\r\n+    return std::string(result.data());\r\n+}\r\n+\r\n+struct llama_token_data_with_prob {\r\n+    llama_token tok;\r\n+    float prob;\r\n+};\r\n+\r\n+struct completion_token_output {\r\n+    llama_token tok;\r\n+    std::vector<llama_token_data_with_prob> probs;\r\n+};\r\n+\r\n+// Sampling parameters structure\r\n+struct sampling_params {\r\n+    int32_t seed;\r\n+    int32_t n_predict;\r\n+    int32_t top_k;\r\n+    float top_p;\r\n+    float temp;\r\n+    float penalty_repeat;\r\n+    int32_t penalty_last_n;\r\n+    bool ignore_eos;\r\n+    std::vector<std::string> antiprompt;\r\n+};\r\n+\r\n+// CPU parameters structure\r\n+struct cpu_params {\r\n+    int32_t n_threads;\r\n+};\r\n+\r\n+// Parameters structure\r\n+struct params {\r\n+    std::string prompt;\r\n+    sampling_params sparams;\r\n+    cpu_params cpuparams;\r\n+    int32_t n_predict;\r\n+};\r\n+\r\n+class llama_rn_context {\r\n+public:  // Make these public since they're accessed from Objective-C\r\n+    llama_model* model;\r\n+    llama_context* ctx;\r\n+    bool is_load_interrupted;\r\n+    int loading_progress;\r\n+    bool is_predicting;\r\n+    bool is_interrupted;\r\n+    bool has_next_token;\r\n+    std::string last_error;\r\n+    params params;\r\n+\r\n+public:\r\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false),\r\n+                        loading_progress(0), is_predicting(false), is_interrupted(false),\r\n+                        has_next_token(false) {\r\n+        // Initialize default parameters\r\n+        params.sparams.seed = -1;\r\n+        params.sparams.n_predict = 128;\r\n+        params.sparams.top_k = 40;\r\n+        params.sparams.top_p = 0.95f;\r\n+        params.sparams.temp = 0.8f;\r\n+        params.sparams.penalty_repeat = 1.1f;\r\n+        params.sparams.penalty_last_n = 64;\r\n+        params.sparams.ignore_eos = false;\r\n+        params.cpuparams.n_threads = 4;\r\n+    }\r\n+\r\n+    ~llama_rn_context() {\r\n+        if (ctx) llama_free(ctx);\r\n+        if (model) llama_free_model(model);\r\n+    }\r\n+\r\n+    bool load_model(const common_params& params, NSString** error) {\r\n+        try {\r\n+            // Check file access\r\n+            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n+            if (!f) {\r\n+                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+            fclose(f);\r\n+\r\n+            // Load model\r\n+            llama_model_params model_params = llama_model_default_params();\r\n+            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n+\r\n+            if (!model) {\r\n+                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+\r\n+            // Initialize context\r\n+            llama_context_params ctx_params = llama_context_default_params();\r\n+            ctx_params.n_ctx = params.n_ctx;\r\n+            ctx = llama_new_context_with_model(model, ctx_params);\r\n+\r\n+            if (!ctx) {\r\n+                last_error = \"Failed to create context\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                llama_free_model(model);\r\n+                model = nullptr;\r\n+                return false;\r\n+            }\r\n+\r\n+            return true;\r\n+\r\n+        } catch (const std::exception& e) {\r\n+            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n+            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+            return false;\r\n+        }\r\n+    }\r\n+\r\n+    void rewind() {\r\n+        if (ctx) {\r\n+            llama_kv_cache_clear(ctx);\r\n+        }\r\n+    }\r\n+\r\n+    bool initSampling() {\r\n+        if (!ctx || !model) return false;\r\n+        has_next_token = true;\r\n+        is_interrupted = false;\r\n+        return true;\r\n+    }\r\n+\r\n+    void beginCompletion() {\r\n+        is_predicting = true;\r\n+    }\r\n+\r\n+    void loadPrompt() {\r\n+        if (!ctx || params.prompt.empty()) return;\r\n+\r\n+        // Allocate token buffer\r\n+        std::vector<llama_token> tokens(params.prompt.size() + 1);\r\n+        int n_tokens = llama_tokenize(\r\n+            model,\r\n+            params.prompt.c_str(),\r\n+            params.prompt.length(),\r\n+            tokens.data(),\r\n+            tokens.size(),\r\n+            true,\r\n+            false\r\n+        );\r\n+\r\n+        if (n_tokens < 0) {\r\n+            tokens.resize(-n_tokens);\r\n+            n_tokens = llama_tokenize(\r\n+                model,\r\n+                params.prompt.c_str(),\r\n+                params.prompt.length(),\r\n+                tokens.data(),\r\n+                tokens.size(),\r\n+                true,\r\n+                false\r\n+            );\r\n+        }\r\n+\r\n+        if (n_tokens > 0) {\r\n+            tokens.resize(n_tokens);\r\n+            llama_decode(ctx, llama_batch_get_one(tokens.data(), tokens.size(), 0, 0));\r\n+        }\r\n+    }\r\n+\r\n+    bool validateModelChatTemplate() const {\r\n+        return true;\r\n+    }\r\n+\r\n+    int applyLoraAdapters(const std::vector<common_lora_adapter_info>& adapters) {\r\n+        return 0;\r\n+    }\r\n+\r\n+    // Getter methods for compatibility\r\n+    llama_model* get_model() const { return model; }\r\n+    llama_context* get_context() const { return ctx; }\r\n+    bool get_is_load_interrupted() const { return is_load_interrupted; }\r\n+    void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\r\n+    int get_loading_progress() const { return loading_progress; }\r\n+    void set_loading_progress(int value) { loading_progress = value; }\r\n+};\r\n+\r\n+} // namespace rnllama\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733074758101,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,175 @@\n+#pragma once\n+\n+#include <string>\n+#include <exception>\n+#include <vector>\n+#include \"llama.h\"\n+#include \"common.h\"\n+#include \"ggml.h\"\n+\n+namespace rnllama {\n+\n+// Helper function to convert GGUF key-value to string\n+inline std::string lm_gguf_kv_to_str(struct lm_gguf_context* ctx, int i) {\n+    return std::string(lm_gguf_get_val_str(ctx, i));\n+}\n+\n+// Helper function to format tokens to string\n+inline std::string tokens_to_output_formatted_string(llama_context* ctx, llama_token token) {\n+    const char* str = llama_token_to_str(ctx, token);\n+    return std::string(str ? str : \"\");\n+}\n+\n+struct llama_token_data_with_prob {\n+    llama_token tok;\n+    float prob;\n+};\n+\n+struct completion_token_output {\n+    llama_token tok;\n+    std::vector<llama_token_data_with_prob> probs;\n+};\n+\n+// Sampling parameters structure\n+struct sampling_params {\n+    int32_t seed;\n+    int32_t n_predict;\n+    int32_t top_k;\n+    float top_p;\n+    float temp;\n+    float penalty_repeat;\n+    int32_t penalty_last_n;\n+    bool ignore_eos;\n+    std::vector<std::string> antiprompt;\n+};\n+\n+// CPU parameters structure\n+struct cpu_params {\n+    int32_t n_threads;\n+};\n+\n+// Parameters structure\n+struct params {\n+    std::string prompt;\n+    sampling_params sparams;\n+    cpu_params cpuparams;\n+    int32_t n_predict;\n+};\n+\n+class llama_rn_context {\n+public:  // Make these public since they're accessed from Objective-C\n+    llama_model* model;\n+    llama_context* ctx;\n+    bool is_load_interrupted;\n+    int loading_progress;\n+    bool is_predicting;\n+    bool is_interrupted;\n+    bool has_next_token;\n+    std::string last_error;\n+    params params;\n+\n+public:\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false),\n+                        loading_progress(0), is_predicting(false), is_interrupted(false),\n+                        has_next_token(false) {\n+        // Initialize default parameters\n+        params.sparams.seed = -1;\n+        params.sparams.n_predict = 128;\n+        params.sparams.top_k = 40;\n+        params.sparams.top_p = 0.95f;\n+        params.sparams.temp = 0.8f;\n+        params.sparams.penalty_repeat = 1.1f;\n+        params.sparams.penalty_last_n = 64;\n+        params.sparams.ignore_eos = false;\n+        params.cpuparams.n_threads = 4;\n+    }\n+\n+    ~llama_rn_context() {\n+        if (ctx) llama_free(ctx);\n+        if (model) llama_free_model(model);\n+    }\n+\n+    bool load_model(const common_params& params, NSString** error) {\n+        try {\n+            // Check file access\n+            FILE* f = fopen(params.model.c_str(), \"rb\");\n+            if (!f) {\n+                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+                return false;\n+            }\n+            fclose(f);\n+\n+            // Load model\n+            llama_model_params model_params = llama_model_default_params();\n+            model = llama_load_model_from_file(params.model.c_str(), model_params);\n+\n+            if (!model) {\n+                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+                return false;\n+            }\n+\n+            // Initialize context\n+            llama_context_params ctx_params = llama_context_default_params();\n+            ctx_params.n_ctx = params.n_ctx;\n+            ctx = llama_new_context_with_model(model, ctx_params);\n+\n+            if (!ctx) {\n+                last_error = \"Failed to create context\";\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+                llama_free_model(model);\n+                model = nullptr;\n+                return false;\n+            }\n+\n+            return true;\n+\n+        } catch (const std::exception& e) {\n+            last_error = std::string(\"Exception during model loading: \") + e.what();\n+            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+            return false;\n+        }\n+    }\n+\n+    void rewind() {\n+        if (ctx) {\n+            llama_kv_cache_clear(ctx);\n+        }\n+    }\n+\n+    bool initSampling() {\n+        if (!ctx || !model) return false;\n+        has_next_token = true;\n+        is_interrupted = false;\n+        return true;\n+    }\n+\n+    void beginCompletion() {\n+        is_predicting = true;\n+    }\n+\n+    void loadPrompt() {\n+        if (!ctx || params.prompt.empty()) return;\n+        std::vector<llama_token> tokens = llama_tokenize(ctx, params.prompt.c_str(), true);\n+        llama_eval(ctx, tokens.data(), tokens.size(), 0, params.cpuparams.n_threads);\n+    }\n+\n+    bool validateModelChatTemplate() const {\n+        return true;\n+    }\n+\n+    int applyLoraAdapters(const std::vector<common_lora_adapter_info>& adapters) {\n+        return 0;\n+    }\n+\n+    // Getter methods for compatibility\n+    llama_model* get_model() const { return model; }\n+    llama_context* get_context() const { return ctx; }\n+    bool get_is_load_interrupted() const { return is_load_interrupted; }\n+    void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\n+    int get_loading_progress() const { return loading_progress; }\n+    void set_loading_progress(int value) { loading_progress = value; }\n+};\n+\n+} // namespace rnllama\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733075045005,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,206 @@\n+#pragma once\n+\n+#include <string>\n+#include <exception>\n+#include <vector>\n+#include \"llama.h\"\n+#include \"common.h\"\n+#include \"ggml.h\"\n+\n+namespace rnllama {\n+\n+// Helper function to convert GGUF key-value to string\n+inline std::string lm_gguf_kv_to_str(struct lm_gguf_context* ctx, int i) {\n+    return std::string(lm_gguf_get_val_str(ctx, i));\n+}\n+\n+// Helper function to format tokens to string\n+inline std::string tokens_to_output_formatted_string(llama_context* ctx, llama_token token) {\n+    std::vector<char> result(8, 0);\n+    const int n_tokens = llama_token_to_piece(ctx, token, result.data(), result.size());\n+    if (n_tokens < 0) {\n+        result.resize(-n_tokens);\n+        llama_token_to_piece(ctx, token, result.data(), result.size());\n+    }\n+    return std::string(result.data());\n+}\n+\n+struct llama_token_data_with_prob {\n+    llama_token tok;\n+    float prob;\n+};\n+\n+struct completion_token_output {\n+    llama_token tok;\n+    std::vector<llama_token_data_with_prob> probs;\n+};\n+\n+// Sampling parameters structure\n+struct sampling_params {\n+    int32_t seed;\n+    int32_t n_predict;\n+    int32_t top_k;\n+    float top_p;\n+    float temp;\n+    float penalty_repeat;\n+    int32_t penalty_last_n;\n+    bool ignore_eos;\n+    std::vector<std::string> antiprompt;\n+};\n+\n+// CPU parameters structure\n+struct cpu_params {\n+    int32_t n_threads;\n+};\n+\n+// Parameters structure\n+struct params {\n+    std::string prompt;\n+    sampling_params sparams;\n+    cpu_params cpuparams;\n+    int32_t n_predict;\n+};\n+\n+class llama_rn_context {\n+public:  // Make these public since they're accessed from Objective-C\n+    llama_model* model;\n+    llama_context* ctx;\n+    bool is_load_interrupted;\n+    int loading_progress;\n+    bool is_predicting;\n+    bool is_interrupted;\n+    bool has_next_token;\n+    std::string last_error;\n+    params params;\n+\n+public:\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false),\n+                        loading_progress(0), is_predicting(false), is_interrupted(false),\n+                        has_next_token(false) {\n+        // Initialize default parameters\n+        params.sparams.seed = -1;\n+        params.sparams.n_predict = 128;\n+        params.sparams.top_k = 40;\n+        params.sparams.top_p = 0.95f;\n+        params.sparams.temp = 0.8f;\n+        params.sparams.penalty_repeat = 1.1f;\n+        params.sparams.penalty_last_n = 64;\n+        params.sparams.ignore_eos = false;\n+        params.cpuparams.n_threads = 4;\n+    }\n+\n+    ~llama_rn_context() {\n+        if (ctx) llama_free(ctx);\n+        if (model) llama_free_model(model);\n+    }\n+\n+    bool load_model(const common_params& params, NSString** error) {\n+        try {\n+            // Check file access\n+            FILE* f = fopen(params.model.c_str(), \"rb\");\n+            if (!f) {\n+                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+                return false;\n+            }\n+            fclose(f);\n+\n+            // Load model\n+            llama_model_params model_params = llama_model_default_params();\n+            model = llama_load_model_from_file(params.model.c_str(), model_params);\n+\n+            if (!model) {\n+                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+                return false;\n+            }\n+\n+            // Initialize context\n+            llama_context_params ctx_params = llama_context_default_params();\n+            ctx_params.n_ctx = params.n_ctx;\n+            ctx = llama_new_context_with_model(model, ctx_params);\n+\n+            if (!ctx) {\n+                last_error = \"Failed to create context\";\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+                llama_free_model(model);\n+                model = nullptr;\n+                return false;\n+            }\n+\n+            return true;\n+\n+        } catch (const std::exception& e) {\n+            last_error = std::string(\"Exception during model loading: \") + e.what();\n+            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\n+            return false;\n+        }\n+    }\n+\n+    void rewind() {\n+        if (ctx) {\n+            llama_kv_cache_clear(ctx);\n+        }\n+    }\n+\n+    bool initSampling() {\n+        if (!ctx || !model) return false;\n+        has_next_token = true;\n+        is_interrupted = false;\n+        return true;\n+    }\n+\n+    void beginCompletion() {\n+        is_predicting = true;\n+    }\n+\n+    void loadPrompt() {\n+        if (!ctx || params.prompt.empty()) return;\n+\n+        // Get max token count\n+        const int max_tokens = llama_n_ctx(ctx);\n+        std::vector<llama_token> tokens(max_tokens);\n+\n+        // Tokenize the prompt\n+        int n_tokens = llama_tokenize(\n+            model,\n+            params.prompt.c_str(),\n+            params.prompt.length(),\n+            tokens.data(),\n+            max_tokens,\n+            true,  // add_bos\n+            false  // special tokens\n+        );\n+\n+        if (n_tokens < 0) {\n+            // Handle error\n+            return;\n+        }\n+\n+        tokens.resize(n_tokens);\n+\n+        // Evaluate the tokens\n+        if (llama_decode(ctx, llama_batch_get_one(tokens.data(), tokens.size(), 0, 0)) != 0) {\n+            // Handle error\n+            return;\n+        }\n+    }\n+\n+    bool validateModelChatTemplate() const {\n+        return true;\n+    }\n+\n+    int applyLoraAdapters(const std::vector<common_lora_adapter_info>& adapters) {\n+        return 0;\n+    }\n+\n+    // Getter methods for compatibility\n+    llama_model* get_model() const { return model; }\n+    llama_context* get_context() const { return ctx; }\n+    bool get_is_load_interrupted() const { return is_load_interrupted; }\n+    void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\n+    int get_loading_progress() const { return loading_progress; }\n+    void set_loading_progress(int value) { loading_progress = value; }\n+};\n+\n+} // namespace rnllama\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733070571467,
            "name": "Commit-0",
            "content": "class llama_rn_context {\r\nprivate:\r\n    llama_model* model;\r\n    llama_context* ctx;\r\n    bool is_load_interrupted;\r\n    int loading_progress;\r\n    std::string last_error;\r\n\r\npublic:\r\n    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n\r\n    bool load_model(const common_params& params, NSString** error) {\r\n        try {\r\n            // Check file access\r\n            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n            if (!f) {\r\n                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                return false;\r\n            }\r\n            fclose(f);\r\n\r\n            // Load model\r\n            llama_model_params model_params = llama_model_default_params();\r\n            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n\r\n            if (!model) {\r\n                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                return false;\r\n            }\r\n\r\n            // Initialize context\r\n            llama_context_params ctx_params = llama_context_default_params();\r\n            ctx_params.n_ctx = params.n_ctx;\r\n            ctx = llama_new_context_with_model(model, ctx_params);\r\n\r\n            if (!ctx) {\r\n                last_error = \"Failed to create context\";\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                llama_free_model(model);\r\n                model = nullptr;\r\n                return false;\r\n            }\r\n\r\n            return true;\r\n\r\n        } catch (const std::exception& e) {\r\n            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n            return false;\r\n        }\r\n    }\r\n\r\n    // ... rest of the class implementation ...\r\n};"
        }
    ]
}
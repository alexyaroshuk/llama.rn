{
    "sourceFile": "ios/rn-llama.hpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1733070571467,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733072616214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,83 @@\n+#pragma once\r\n+\r\n+#include <string>\r\n+#include <exception>\r\n+#include <vector>\r\n+#include \"llama.h\"\r\n+#include \"common.h\"\r\n+\r\n+namespace rnllama {\r\n+\r\n+struct completion_token_output {\r\n+    llama_token tok;\r\n+    std::vector<llama_token_data> probs;\r\n+};\r\n+\r\n+class llama_rn_context {\r\n+private:\r\n+    llama_model* model;\r\n+    llama_context* ctx;\r\n+    bool is_load_interrupted;\r\n+    int loading_progress;\r\n+    std::string last_error;\r\n+\r\n+public:\r\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n+\r\n+    ~llama_rn_context() {\r\n+        if (ctx) llama_free(ctx);\r\n+        if (model) llama_free_model(model);\r\n+    }\r\n+\r\n+    bool load_model(const common_params& params, NSString** error) {\r\n+        try {\r\n+            // Check file access\r\n+            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n+            if (!f) {\r\n+                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+            fclose(f);\r\n+\r\n+            // Load model\r\n+            llama_model_params model_params = llama_model_default_params();\r\n+            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n+\r\n+            if (!model) {\r\n+                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+\r\n+            // Initialize context\r\n+            llama_context_params ctx_params = llama_context_default_params();\r\n+            ctx_params.n_ctx = params.n_ctx;\r\n+            ctx = llama_new_context_with_model(model, ctx_params);\r\n+\r\n+            if (!ctx) {\r\n+                last_error = \"Failed to create context\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                llama_free_model(model);\r\n+                model = nullptr;\r\n+                return false;\r\n+            }\r\n+\r\n+            return true;\r\n+\r\n+        } catch (const std::exception& e) {\r\n+            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n+            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+            return false;\r\n+        }\r\n+    }\r\n+\r\n+    llama_model* get_model() const { return model; }\r\n+    llama_context* get_context() const { return ctx; }\r\n+    bool get_is_load_interrupted() const { return is_load_interrupted; }\r\n+    void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\r\n+    int get_loading_progress() const { return loading_progress; }\r\n+    void set_loading_progress(int value) { loading_progress = value; }\r\n+};\r\n+\r\n+} // namespace rnllama\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733070571467,
            "name": "Commit-0",
            "content": "class llama_rn_context {\r\nprivate:\r\n    llama_model* model;\r\n    llama_context* ctx;\r\n    bool is_load_interrupted;\r\n    int loading_progress;\r\n    std::string last_error;\r\n\r\npublic:\r\n    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n\r\n    bool load_model(const common_params& params, NSString** error) {\r\n        try {\r\n            // Check file access\r\n            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n            if (!f) {\r\n                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                return false;\r\n            }\r\n            fclose(f);\r\n\r\n            // Load model\r\n            llama_model_params model_params = llama_model_default_params();\r\n            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n\r\n            if (!model) {\r\n                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                return false;\r\n            }\r\n\r\n            // Initialize context\r\n            llama_context_params ctx_params = llama_context_default_params();\r\n            ctx_params.n_ctx = params.n_ctx;\r\n            ctx = llama_new_context_with_model(model, ctx_params);\r\n\r\n            if (!ctx) {\r\n                last_error = \"Failed to create context\";\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                llama_free_model(model);\r\n                model = nullptr;\r\n                return false;\r\n            }\r\n\r\n            return true;\r\n\r\n        } catch (const std::exception& e) {\r\n            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n            return false;\r\n        }\r\n    }\r\n\r\n    // ... rest of the class implementation ...\r\n};"
        }
    ]
}
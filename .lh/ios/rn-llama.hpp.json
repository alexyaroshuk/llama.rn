{
    "sourceFile": "ios/rn-llama.hpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1733070571467,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733072616214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,83 @@\n+#pragma once\r\n+\r\n+#include <string>\r\n+#include <exception>\r\n+#include <vector>\r\n+#include \"llama.h\"\r\n+#include \"common.h\"\r\n+\r\n+namespace rnllama {\r\n+\r\n+struct completion_token_output {\r\n+    llama_token tok;\r\n+    std::vector<llama_token_data> probs;\r\n+};\r\n+\r\n+class llama_rn_context {\r\n+private:\r\n+    llama_model* model;\r\n+    llama_context* ctx;\r\n+    bool is_load_interrupted;\r\n+    int loading_progress;\r\n+    std::string last_error;\r\n+\r\n+public:\r\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n+\r\n+    ~llama_rn_context() {\r\n+        if (ctx) llama_free(ctx);\r\n+        if (model) llama_free_model(model);\r\n+    }\r\n+\r\n+    bool load_model(const common_params& params, NSString** error) {\r\n+        try {\r\n+            // Check file access\r\n+            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n+            if (!f) {\r\n+                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+            fclose(f);\r\n+\r\n+            // Load model\r\n+            llama_model_params model_params = llama_model_default_params();\r\n+            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n+\r\n+            if (!model) {\r\n+                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                return false;\r\n+            }\r\n+\r\n+            // Initialize context\r\n+            llama_context_params ctx_params = llama_context_default_params();\r\n+            ctx_params.n_ctx = params.n_ctx;\r\n+            ctx = llama_new_context_with_model(model, ctx_params);\r\n+\r\n+            if (!ctx) {\r\n+                last_error = \"Failed to create context\";\r\n+                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+                llama_free_model(model);\r\n+                model = nullptr;\r\n+                return false;\r\n+            }\r\n+\r\n+            return true;\r\n+\r\n+        } catch (const std::exception& e) {\r\n+            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n+            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n+            return false;\r\n+        }\r\n+    }\r\n+\r\n+    llama_model* get_model() const { return model; }\r\n+    llama_context* get_context() const { return ctx; }\r\n+    bool get_is_load_interrupted() const { return is_load_interrupted; }\r\n+    void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\r\n+    int get_loading_progress() const { return loading_progress; }\r\n+    void set_loading_progress(int value) { loading_progress = value; }\r\n+};\r\n+\r\n+} // namespace rnllama\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733073426866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,26 +4,34 @@\n #include <exception>\r\n #include <vector>\r\n #include \"llama.h\"\r\n #include \"common.h\"\r\n+#include \"ggml.h\"\r\n \r\n namespace rnllama {\r\n \r\n+// Helper function to convert GGUF key-value to string\r\n+inline std::string lm_gguf_kv_to_str(struct lm_gguf_context* ctx, int i) {\r\n+    return std::string(lm_gguf_get_val_str(ctx, i));\r\n+}\r\n+\r\n struct completion_token_output {\r\n     llama_token tok;\r\n     std::vector<llama_token_data> probs;\r\n };\r\n \r\n class llama_rn_context {\r\n-private:\r\n+public:  // Make these public since they're accessed from Objective-C\r\n     llama_model* model;\r\n     llama_context* ctx;\r\n     bool is_load_interrupted;\r\n     int loading_progress;\r\n+    bool is_predicting;\r\n     std::string last_error;\r\n \r\n public:\r\n-    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n+    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false),\r\n+                        loading_progress(0), is_predicting(false) {}\r\n \r\n     ~llama_rn_context() {\r\n         if (ctx) llama_free(ctx);\r\n         if (model) llama_free_model(model);\r\n@@ -71,8 +79,19 @@\n             return false;\r\n         }\r\n     }\r\n \r\n+    bool validateModelChatTemplate() const {\r\n+        // Add implementation based on your requirements\r\n+        return true;\r\n+    }\r\n+\r\n+    int applyLoraAdapters(const std::vector<common_lora_adapter_info>& adapters) {\r\n+        // Add implementation based on your requirements\r\n+        return 0;\r\n+    }\r\n+\r\n+    // Getter methods for compatibility\r\n     llama_model* get_model() const { return model; }\r\n     llama_context* get_context() const { return ctx; }\r\n     bool get_is_load_interrupted() const { return is_load_interrupted; }\r\n     void set_is_load_interrupted(bool value) { is_load_interrupted = value; }\r\n"
                },
                {
                    "date": 1733074158325,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,25 +13,77 @@\n inline std::string lm_gguf_kv_to_str(struct lm_gguf_context* ctx, int i) {\r\n     return std::string(lm_gguf_get_val_str(ctx, i));\r\n }\r\n \r\n+// Helper function to format tokens to string\r\n+inline std::string tokens_to_output_formatted_string(llama_context* ctx, llama_token token) {\r\n+    const char* str = llama_token_to_str(ctx, token);\r\n+    return std::string(str ? str : \"\");\r\n+}\r\n+\r\n+struct llama_token_data_with_prob {\r\n+    llama_token tok;\r\n+    float prob;\r\n+};\r\n+\r\n struct completion_token_output {\r\n     llama_token tok;\r\n-    std::vector<llama_token_data> probs;\r\n+    std::vector<llama_token_data_with_prob> probs;\r\n };\r\n \r\n+// Sampling parameters structure\r\n+struct sampling_params {\r\n+    int32_t seed;\r\n+    int32_t n_predict;\r\n+    int32_t top_k;\r\n+    float top_p;\r\n+    float temp;\r\n+    float penalty_repeat;\r\n+    int32_t penalty_last_n;\r\n+    bool ignore_eos;\r\n+    std::vector<std::string> antiprompt;\r\n+};\r\n+\r\n+// CPU parameters structure\r\n+struct cpu_params {\r\n+    int32_t n_threads;\r\n+};\r\n+\r\n+// Parameters structure\r\n+struct params {\r\n+    std::string prompt;\r\n+    sampling_params sparams;\r\n+    cpu_params cpuparams;\r\n+    int32_t n_predict;\r\n+};\r\n+\r\n class llama_rn_context {\r\n public:  // Make these public since they're accessed from Objective-C\r\n     llama_model* model;\r\n     llama_context* ctx;\r\n     bool is_load_interrupted;\r\n     int loading_progress;\r\n     bool is_predicting;\r\n+    bool is_interrupted;\r\n+    bool has_next_token;\r\n     std::string last_error;\r\n+    params params;\r\n \r\n public:\r\n     llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false),\r\n-                        loading_progress(0), is_predicting(false) {}\r\n+                        loading_progress(0), is_predicting(false), is_interrupted(false),\r\n+                        has_next_token(false) {\r\n+        // Initialize default parameters\r\n+        params.sparams.seed = -1;\r\n+        params.sparams.n_predict = 128;\r\n+        params.sparams.top_k = 40;\r\n+        params.sparams.top_p = 0.95f;\r\n+        params.sparams.temp = 0.8f;\r\n+        params.sparams.penalty_repeat = 1.1f;\r\n+        params.sparams.penalty_last_n = 64;\r\n+        params.sparams.ignore_eos = false;\r\n+        params.cpuparams.n_threads = 4;\r\n+    }\r\n \r\n     ~llama_rn_context() {\r\n         if (ctx) llama_free(ctx);\r\n         if (model) llama_free_model(model);\r\n@@ -79,15 +131,36 @@\n             return false;\r\n         }\r\n     }\r\n \r\n+    void rewind() {\r\n+        if (ctx) {\r\n+            llama_kv_cache_clear(ctx);\r\n+        }\r\n+    }\r\n+\r\n+    bool initSampling() {\r\n+        if (!ctx || !model) return false;\r\n+        has_next_token = true;\r\n+        is_interrupted = false;\r\n+        return true;\r\n+    }\r\n+\r\n+    void beginCompletion() {\r\n+        is_predicting = true;\r\n+    }\r\n+\r\n+    void loadPrompt() {\r\n+        if (!ctx || params.prompt.empty()) return;\r\n+        std::vector<llama_token> tokens = llama_tokenize(ctx, params.prompt.c_str(), true);\r\n+        llama_eval(ctx, tokens.data(), tokens.size(), 0, params.cpuparams.n_threads);\r\n+    }\r\n+\r\n     bool validateModelChatTemplate() const {\r\n-        // Add implementation based on your requirements\r\n         return true;\r\n     }\r\n \r\n     int applyLoraAdapters(const std::vector<common_lora_adapter_info>& adapters) {\r\n-        // Add implementation based on your requirements\r\n         return 0;\r\n     }\r\n \r\n     // Getter methods for compatibility\r\n@@ -98,61 +171,5 @@\n     int get_loading_progress() const { return loading_progress; }\r\n     void set_loading_progress(int value) { loading_progress = value; }\r\n };\r\n \r\n-} // namespace rnllama\n-class llama_rn_context {\r\n-private:\r\n-    llama_model* model;\r\n-    llama_context* ctx;\r\n-    bool is_load_interrupted;\r\n-    int loading_progress;\r\n-    std::string last_error;\r\n-\r\n-public:\r\n-    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n-\r\n-    bool load_model(const common_params& params, NSString** error) {\r\n-        try {\r\n-            // Check file access\r\n-            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n-            if (!f) {\r\n-                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n-                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-                return false;\r\n-            }\r\n-            fclose(f);\r\n-\r\n-            // Load model\r\n-            llama_model_params model_params = llama_model_default_params();\r\n-            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n-\r\n-            if (!model) {\r\n-                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n-                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-                return false;\r\n-            }\r\n-\r\n-            // Initialize context\r\n-            llama_context_params ctx_params = llama_context_default_params();\r\n-            ctx_params.n_ctx = params.n_ctx;\r\n-            ctx = llama_new_context_with_model(model, ctx_params);\r\n-\r\n-            if (!ctx) {\r\n-                last_error = \"Failed to create context\";\r\n-                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-                llama_free_model(model);\r\n-                model = nullptr;\r\n-                return false;\r\n-            }\r\n-\r\n-            return true;\r\n-\r\n-        } catch (const std::exception& e) {\r\n-            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n-            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n-            return false;\r\n-        }\r\n-    }\r\n-\r\n-    // ... rest of the class implementation ...\r\n-};\n\\ No newline at end of file\n+} // namespace rnllama\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733070571467,
            "name": "Commit-0",
            "content": "class llama_rn_context {\r\nprivate:\r\n    llama_model* model;\r\n    llama_context* ctx;\r\n    bool is_load_interrupted;\r\n    int loading_progress;\r\n    std::string last_error;\r\n\r\npublic:\r\n    llama_rn_context() : model(nullptr), ctx(nullptr), is_load_interrupted(false), loading_progress(0) {}\r\n\r\n    bool load_model(const common_params& params, NSString** error) {\r\n        try {\r\n            // Check file access\r\n            FILE* f = fopen(params.model.c_str(), \"rb\");\r\n            if (!f) {\r\n                last_error = \"Cannot open model file: \" + std::string(strerror(errno));\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                return false;\r\n            }\r\n            fclose(f);\r\n\r\n            // Load model\r\n            llama_model_params model_params = llama_model_default_params();\r\n            model = llama_load_model_from_file(params.model.c_str(), model_params);\r\n\r\n            if (!model) {\r\n                last_error = \"Failed to load model: llama_load_model_from_file returned null\";\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                return false;\r\n            }\r\n\r\n            // Initialize context\r\n            llama_context_params ctx_params = llama_context_default_params();\r\n            ctx_params.n_ctx = params.n_ctx;\r\n            ctx = llama_new_context_with_model(model, ctx_params);\r\n\r\n            if (!ctx) {\r\n                last_error = \"Failed to create context\";\r\n                if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n                llama_free_model(model);\r\n                model = nullptr;\r\n                return false;\r\n            }\r\n\r\n            return true;\r\n\r\n        } catch (const std::exception& e) {\r\n            last_error = std::string(\"Exception during model loading: \") + e.what();\r\n            if (error) *error = [NSString stringWithUTF8String:last_error.c_str()];\r\n            return false;\r\n        }\r\n    }\r\n\r\n    // ... rest of the class implementation ...\r\n};"
        }
    ]
}
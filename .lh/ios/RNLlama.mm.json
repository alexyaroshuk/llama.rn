{
    "sourceFile": "ios/RNLlama.mm",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1733063024563,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1733063024563,
            "name": "Commit-0",
            "content": "#import \"RNLlama.h\"\r\n#import \"RNLlamaContext.h\"\r\n\r\n#ifdef RCT_NEW_ARCH_ENABLED\r\n#import \"RNLlamaSpec.h\"\r\n#endif\r\n\r\n@implementation RNLlama\r\n\r\nNSMutableDictionary *llamaContexts;\r\ndouble llamaContextLimit = 1;\r\ndispatch_queue_t llamaDQueue;\r\n\r\nRCT_EXPORT_MODULE()\r\n\r\nRCT_EXPORT_METHOD(setContextLimit:(double)limit\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    llamaContextLimit = limit;\r\n    resolve(nil);\r\n}\r\n\r\nRCT_EXPORT_METHOD(modelInfo:(NSString *)path\r\n                 withSkip:(NSArray *)skip\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    resolve([RNLlamaContext modelInfo:path skip:skip]);\r\n}\r\n\r\nRCT_EXPORT_METHOD(initContext:(double)contextId\r\n                 withContextParams:(NSDictionary *)contextParams\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    NSNumber *contextIdNumber = [NSNumber numberWithDouble:contextId];\r\n    if (llamaContexts[contextIdNumber] != nil) {\r\n        reject(@\"llama_error\", @\"Context already exists\", nil);\r\n        return;\r\n    }\r\n\r\n    if (llamaDQueue == nil) {\r\n      llamaDQueue = dispatch_queue_create(\"com.rnllama\", DISPATCH_QUEUE_SERIAL);\r\n    }\r\n\r\n    if (llamaContexts == nil) {\r\n        llamaContexts = [[NSMutableDictionary alloc] init];\r\n    }\r\n\r\n    if (llamaContextLimit > 0 && [llamaContexts count] >= llamaContextLimit) {\r\n        reject(@\"llama_error\", @\"Context limit reached\", nil);\r\n        return;\r\n    }\r\n\r\n    @try {\r\n        NSString *modelPath = [contextParams objectForKey:@\"model\"];\r\n        NSLog(@\"[RNLlama] Attempting to load model from path: %@\", modelPath);\r\n\r\n        BOOL fileExists = [[NSFileManager defaultManager] fileExistsAtPath:modelPath];\r\n        if (!fileExists) {\r\n            NSString *errorMsg = [NSString stringWithFormat:@\"Model file not found at path: %@\", modelPath];\r\n            NSLog(@\"[RNLlama] Error: %@\", errorMsg);\r\n            reject(@\"llama_error\", errorMsg, nil);\r\n            return;\r\n        }\r\n\r\n        NSError *attributesError = nil;\r\n        NSDictionary *attributes = [[NSFileManager defaultManager] attributesOfItemAtPath:modelPath error:&attributesError];\r\n        if (attributes) {\r\n            unsigned long long fileSize = [attributes fileSize];\r\n            NSLog(@\"[RNLlama] Model file size: %llu bytes\", fileSize);\r\n        } else {\r\n            NSLog(@\"[RNLlama] Warning: Could not get model file size: %@\", attributesError);\r\n        }\r\n\r\n        NSLog(@\"[RNLlama] Initialization parameters: %@\", contextParams);\r\n\r\n        RNLlamaContext *context = [RNLlamaContext initWithParams:contextParams onProgress:^(unsigned int progress) {\r\n            dispatch_async(dispatch_get_main_queue(), ^{\r\n                [self sendEventWithName:@\"@RNLlama_onInitContextProgress\" body:@{ @\"contextId\": @(contextId), @\"progress\": @(progress) }];\r\n                NSLog(@\"[RNLlama] Loading progress: %d%%\", progress);\r\n            });\r\n        }];\r\n\r\n        if (![context isModelLoaded]) {\r\n            NSString *errorMsg = [context getLastError];\r\n            NSString *detailedError = [NSString stringWithFormat:@\"Failed to load the model: %@\", errorMsg ?: @\"Unknown error\"];\r\n            NSLog(@\"[RNLlama] Error: %@\", detailedError);\r\n            reject(@\"llama_cpp_error\", detailedError, nil);\r\n            return;\r\n        }\r\n\r\n        NSLog(@\"[RNLlama] Model loaded successfully\");\r\n        [llamaContexts setObject:context forKey:contextIdNumber];\r\n\r\n        resolve(@{\r\n            @\"gpu\": @([context isMetalEnabled]),\r\n            @\"reasonNoGPU\": [context reasonNoMetal],\r\n            @\"model\": [context modelInfo],\r\n        });\r\n    } @catch (NSException *exception) {\r\n        NSString *detailedError = [NSString stringWithFormat:@\"Exception during model initialization: %@ - %@\",\r\n            exception.name,\r\n            exception.reason];\r\n        NSLog(@\"[RNLlama] Critical Error: %@\", detailedError);\r\n        reject(@\"llama_cpp_error\", detailedError, nil);\r\n    }\r\n}\r\n\r\nRCT_EXPORT_METHOD(getFormattedChat:(double)contextId\r\n                 withMessages:(NSArray *)messages\r\n                 withTemplate:(NSString *)chatTemplate\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    resolve([context getFormattedChat:messages withTemplate:chatTemplate]);\r\n}\r\n\r\nRCT_EXPORT_METHOD(loadSession:(double)contextId\r\n                 withFilePath:(NSString *)filePath\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    if ([context isPredicting]) {\r\n        reject(@\"llama_error\", @\"Context is busy\", nil);\r\n        return;\r\n    }\r\n    dispatch_async(llamaDQueue, ^{\r\n        @try {\r\n            @autoreleasepool {\r\n                resolve([context loadSession:filePath]);\r\n            }\r\n        } @catch (NSException *exception) {\r\n            reject(@\"llama_cpp_error\", exception.reason, nil);\r\n        }\r\n    });\r\n}\r\n\r\nRCT_EXPORT_METHOD(saveSession:(double)contextId\r\n                 withFilePath:(NSString *)filePath\r\n                 withSize:(double)size\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    if ([context isPredicting]) {\r\n        reject(@\"llama_error\", @\"Context is busy\", nil);\r\n        return;\r\n    }\r\n    dispatch_async(llamaDQueue, ^{\r\n        @try {\r\n            @autoreleasepool {\r\n                int count = [context saveSession:filePath size:(int)size];\r\n                resolve(@(count));\r\n            }\r\n        } @catch (NSException *exception) {\r\n            reject(@\"llama_cpp_error\", exception.reason, nil);\r\n        }\r\n    });\r\n}\r\n\r\n- (NSArray *)supportedEvents {\r\n  return@[\r\n    @\"@RNLlama_onInitContextProgress\",\r\n    @\"@RNLlama_onToken\",\r\n  ];\r\n}\r\n\r\nRCT_EXPORT_METHOD(completion:(double)contextId\r\n                 withCompletionParams:(NSDictionary *)completionParams\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    if ([context isPredicting]) {\r\n        reject(@\"llama_error\", @\"Context is busy\", nil);\r\n        return;\r\n    }\r\n    dispatch_async(llamaDQueue, ^{\r\n        @try {\r\n            @autoreleasepool {\r\n                NSDictionary* completionResult = [context completion:completionParams\r\n                    onToken:^(NSMutableDictionary *tokenResult) {\r\n                        if (![completionParams[@\"emit_partial_completion\"] boolValue]) return;\r\n                        dispatch_async(dispatch_get_main_queue(), ^{\r\n                            [self sendEventWithName:@\"@RNLlama_onToken\"\r\n                                body:@{\r\n                                    @\"contextId\": [NSNumber numberWithDouble:contextId],\r\n                                    @\"tokenResult\": tokenResult\r\n                                }\r\n                            ];\r\n                            [tokenResult release];\r\n                        });\r\n                    }\r\n                ];\r\n                resolve(completionResult);\r\n            }\r\n        } @catch (NSException *exception) {\r\n            reject(@\"llama_cpp_error\", exception.reason, nil);\r\n            [context stopCompletion];\r\n        }\r\n    });\r\n\r\n}\r\n\r\nRCT_EXPORT_METHOD(stopCompletion:(double)contextId\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    [context stopCompletion];\r\n    resolve(nil);\r\n}\r\n\r\nRCT_EXPORT_METHOD(tokenize:(double)contextId\r\n                  text:(NSString *)text\r\n                  withResolver:(RCTPromiseResolveBlock)resolve\r\n                  withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    NSMutableArray *tokens = [context tokenize:text];\r\n    resolve(@{ @\"tokens\": tokens });\r\n    [tokens release];\r\n}\r\n\r\nRCT_EXPORT_METHOD(detokenize:(double)contextId\r\n                  tokens:(NSArray *)tokens\r\n                  withResolver:(RCTPromiseResolveBlock)resolve\r\n                  withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    resolve([context detokenize:tokens]);\r\n}\r\n\r\nRCT_EXPORT_METHOD(embedding:(double)contextId\r\n                  text:(NSString *)text\r\n                  params:(NSDictionary *)params\r\n                  withResolver:(RCTPromiseResolveBlock)resolve\r\n                  withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    @try {\r\n        NSDictionary *embedding = [context embedding:text params:params];\r\n        resolve(embedding);\r\n    } @catch (NSException *exception) {\r\n        reject(@\"llama_cpp_error\", exception.reason, nil);\r\n    }\r\n}\r\n\r\nRCT_EXPORT_METHOD(bench:(double)contextId\r\n                  pp:(int)pp\r\n                  tg:(int)tg\r\n                  pl:(int)pl\r\n                  nr:(int)nr\r\n                  withResolver:(RCTPromiseResolveBlock)resolve\r\n                  withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    @try {\r\n        NSString *benchResults = [context bench:pp tg:tg pl:pl nr:nr];\r\n        resolve(benchResults);\r\n    } @catch (NSException *exception) {\r\n        reject(@\"llama_cpp_error\", exception.reason, nil);\r\n    }\r\n}\r\n\r\nRCT_EXPORT_METHOD(applyLoraAdapters:(double)contextId\r\n                 withLoraAdapters:(NSArray *)loraAdapters\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    if ([context isPredicting]) {\r\n        reject(@\"llama_error\", @\"Context is busy\", nil);\r\n        return;\r\n    }\r\n    [context applyLoraAdapters:loraAdapters];\r\n    resolve(nil);\r\n}\r\n\r\nRCT_EXPORT_METHOD(removeLoraAdapters:(double)contextId\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    if ([context isPredicting]) {\r\n        reject(@\"llama_error\", @\"Context is busy\", nil);\r\n        return;\r\n    }\r\n    [context removeLoraAdapters];\r\n    resolve(nil);\r\n}\r\n\r\nRCT_EXPORT_METHOD(getLoadedLoraAdapters:(double)contextId\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    resolve([context getLoadedLoraAdapters]);\r\n}\r\n\r\nRCT_EXPORT_METHOD(releaseContext:(double)contextId\r\n                 withResolver:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    RNLlamaContext *context = llamaContexts[[NSNumber numberWithDouble:contextId]];\r\n    if (context == nil) {\r\n        reject(@\"llama_error\", @\"Context not found\", nil);\r\n        return;\r\n    }\r\n    if (![context isModelLoaded]) {\r\n      [context interruptLoad];\r\n    }\r\n    [context stopCompletion];\r\n    dispatch_barrier_sync(llamaDQueue, ^{});\r\n    [context invalidate];\r\n    [llamaContexts removeObjectForKey:[NSNumber numberWithDouble:contextId]];\r\n    resolve(nil);\r\n}\r\n\r\nRCT_EXPORT_METHOD(releaseAllContexts:(RCTPromiseResolveBlock)resolve\r\n                 withRejecter:(RCTPromiseRejectBlock)reject)\r\n{\r\n    [self invalidate];\r\n    resolve(nil);\r\n}\r\n\r\n\r\n- (void)invalidate {\r\n    if (llamaContexts == nil) {\r\n        return;\r\n    }\r\n\r\n    for (NSNumber *contextId in llamaContexts) {\r\n        RNLlamaContext *context = llamaContexts[contextId];\r\n        [context stopCompletion];\r\n        dispatch_barrier_sync(llamaDQueue, ^{});\r\n        [context invalidate];\r\n    }\r\n\r\n    [llamaContexts removeAllObjects];\r\n    [llamaContexts release];\r\n    llamaContexts = nil;\r\n\r\n    if (llamaDQueue != nil) {\r\n        dispatch_release(llamaDQueue);\r\n        llamaDQueue = nil;\r\n    }\r\n\r\n    [super invalidate];\r\n}\r\n\r\n// Don't compile this code when we build for the old architecture.\r\n#ifdef RCT_NEW_ARCH_ENABLED\r\n- (std::shared_ptr<facebook::react::TurboModule>)getTurboModule:\r\n    (const facebook::react::ObjCTurboModule::InitParams &)params\r\n{\r\n    return std::make_shared<facebook::react::NativeRNLlamaSpecJSI>(params);\r\n}\r\n#endif\r\n\r\n@end\r\n"
        }
    ]
}
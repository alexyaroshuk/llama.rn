{
    "sourceFile": "example/src/App.tsx",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1733070571471,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733070644844,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,579 @@\n+import React, { useState, useRef } from 'react'\r\n+import type { ReactNode } from 'react'\r\n+import { Platform } from 'react-native'\r\n+import { SafeAreaProvider } from 'react-native-safe-area-context'\r\n+import DocumentPicker from 'react-native-document-picker'\r\n+import type { DocumentPickerResponse } from 'react-native-document-picker'\r\n+import { Chat, darkTheme } from '@flyerhq/react-native-chat-ui'\r\n+import type { MessageType } from '@flyerhq/react-native-chat-ui'\r\n+import json5 from 'json5'\r\n+import ReactNativeBlobUtil from 'react-native-blob-util'\r\n+import type { LlamaContext } from 'llama.rn'\r\n+import {\r\n+  initLlama,\r\n+  loadLlamaModelInfo,\r\n+  convertJsonSchemaToGrammar,\r\n+  // eslint-disable-next-line import/no-unresolved\r\n+} from 'llama.rn'\r\n+import { Bubble } from './Bubble'\r\n+\r\n+const { dirs } = ReactNativeBlobUtil.fs\r\n+\r\n+const randId = () => Math.random().toString(36).substr(2, 9)\r\n+\r\n+const user = { id: 'y9d7f8pgn' }\r\n+\r\n+const systemId = 'h3o3lc5xj'\r\n+const system = { id: systemId }\r\n+\r\n+const systemMessage = {\r\n+  role: 'system',\r\n+  content:\r\n+    'This is a conversation between user and assistant, a friendly chatbot.\\n\\n',\r\n+}\r\n+\r\n+const defaultConversationId = 'default'\r\n+\r\n+const renderBubble = ({\r\n+  child,\r\n+  message,\r\n+}: {\r\n+  child: ReactNode\r\n+  message: MessageType.Any\r\n+}) => <Bubble child={child} message={message} />\r\n+\r\n+// Add type declarations for messages and state\r\n+type Message = MessageType.Any\r\n+type Messages = Message[]\r\n+\r\n+// Add proper types for error handling\r\n+type LlamaError = {\r\n+  message: string\r\n+}\r\n+\r\n+// Add proper types for session and completion results\r\n+type SessionDetails = {\r\n+  tokens_loaded: number\r\n+}\r\n+\r\n+type CompletionResult = {\r\n+  timings: {\r\n+    predicted_per_token_ms: number\r\n+    predicted_per_second: number\r\n+  }\r\n+}\r\n+\r\n+// Add proper types for message handling\r\n+type MessageData = {\r\n+  token: string\r\n+}\r\n+\r\n+export default function App() {\r\n+  const [context, setContext] = useState<LlamaContext | undefined>(undefined)\r\n+\r\n+  const [inferencing, setInferencing] = useState<boolean>(false)\r\n+  const [messages, setMessages] = useState<Message[]>([])\r\n+\r\n+  const conversationIdRef = useRef<string>(defaultConversationId)\r\n+\r\n+  const addMessage = (message: Message, batching = false) => {\r\n+    if (batching) {\r\n+      setMessages([message, ...messages])\r\n+    } else {\r\n+      setMessages((msgs: Messages) => [message, ...msgs])\r\n+    }\r\n+  }\r\n+\r\n+  const addSystemMessage = (text: string, metadata = {}) => {\r\n+    const textMessage: MessageType.Text = {\r\n+      author: system,\r\n+      createdAt: Date.now(),\r\n+      id: randId(),\r\n+      text,\r\n+      type: 'text',\r\n+      metadata: { system: true, ...metadata },\r\n+    }\r\n+    addMessage(textMessage)\r\n+    return textMessage.id\r\n+  }\r\n+\r\n+  const handleReleaseContext = async () => {\r\n+    if (!context) return\r\n+    addSystemMessage('Releasing context...')\r\n+    context\r\n+      .release()\r\n+      .then(() => {\r\n+        setContext(undefined)\r\n+        addSystemMessage('Context released!')\r\n+      })\r\n+      .catch((err: LlamaError) => {\r\n+        addSystemMessage(`Context release failed: ${err.message}`)\r\n+      })\r\n+  }\r\n+\r\n+  // Example: Get model info without initializing context\r\n+  const getModelInfo = async (model: string) => {\r\n+    const t0 = Date.now()\r\n+    const info = await loadLlamaModelInfo(model)\r\n+    console.log(`Model info (took ${Date.now() - t0}ms): `, info)\r\n+  }\r\n+\r\n+  const handleInitContext = async (\r\n+    file: DocumentPickerResponse,\r\n+    loraFile: DocumentPickerResponse | null,\r\n+  ) => {\r\n+    await handleReleaseContext()\r\n+    await getModelInfo(file.uri)\r\n+    const msgId = addSystemMessage('Initializing context...')\r\n+    const t0 = Date.now()\r\n+    try {\r\n+      const ctx = await initLlama(\r\n+        {\r\n+          model: file.uri,\r\n+          use_mlock: true,\r\n+          n_gpu_layers: Platform.OS === 'ios' ? 99 : 0,\r\n+          use_progress_callback: true,\r\n+          lora_list: loraFile ? [{ path: loraFile.uri, scaled: 1.0 }] : undefined,\r\n+        },\r\n+        (progress: number) => {\r\n+          setMessages((msgs: Messages) => {\r\n+            const index = msgs.findIndex((msg: Message) => msg.id === msgId)\r\n+            if (index >= 0) {\r\n+              return msgs.map((msg: Message, i: number) => {\r\n+                if (msg.type === 'text' && i === index) {\r\n+                  return {\r\n+                    ...msg,\r\n+                    text: `Initializing context... ${progress}%`,\r\n+                  }\r\n+                }\r\n+                return msg\r\n+              })\r\n+            }\r\n+            return msgs\r\n+          })\r\n+        },\r\n+      )\r\n+      const t1 = Date.now()\r\n+      setContext(ctx)\r\n+      addSystemMessage(\r\n+        `Context initialized!\\n\\nLoad time: ${t1 - t0}ms\\nGPU: ${\r\n+          ctx.gpu ? 'YES' : 'NO'\r\n+        } (${ctx.reasonNoGPU})\\nChat Template: ${\r\n+          ctx.model.isChatTemplateSupported ? 'YES' : 'NO'\r\n+        }\\n\\n` +\r\n+          'You can use the following commands:\\n\\n' +\r\n+          '- /info: to get the model info\\n' +\r\n+          '- /bench: to benchmark the model\\n' +\r\n+          '- /release: release the context\\n' +\r\n+          '- /stop: stop the current completion\\n' +\r\n+          '- /reset: reset the conversation' +\r\n+          '- /save-session: save the session tokens\\n' +\r\n+          '- /load-session: load the session tokens',\r\n+      )\r\n+    } catch (err: unknown) {\r\n+      const error = err as LlamaError\r\n+      const errorMsg = error.message || 'Unknown error'\r\n+      addSystemMessage(`Context initialization failed: ${errorMsg}\\n\\nPlease check:\\n1. Model file exists and is accessible\\n2. Sufficient memory available\\n3. Model file is not corrupted`)\r\n+    }\r\n+  }\r\n+\r\n+  const copyFileIfNeeded = async (\r\n+    type = 'model',\r\n+    file: DocumentPickerResponse,\r\n+  ) => {\r\n+    if (Platform.OS === 'android' && file.uri.startsWith('content://')) {\r\n+      const dir = `${ReactNativeBlobUtil.fs.dirs.CacheDir}/${type}s`\r\n+      const filepath = `${dir}/${file.uri.split('/').pop() || type}.gguf`\r\n+\r\n+      if (!(await ReactNativeBlobUtil.fs.isDir(dir)))\r\n+        await ReactNativeBlobUtil.fs.mkdir(dir)\r\n+\r\n+      if (await ReactNativeBlobUtil.fs.exists(filepath))\r\n+        return { uri: filepath } as DocumentPickerResponse\r\n+\r\n+      await ReactNativeBlobUtil.fs.unlink(dir) // Clean up old files in models\r\n+\r\n+      addSystemMessage(`Copying ${type} to internal storage...`)\r\n+      await ReactNativeBlobUtil.MediaCollection.copyToInternal(\r\n+        file.uri,\r\n+        filepath,\r\n+      )\r\n+      addSystemMessage(`${type} copied!`)\r\n+      return { uri: filepath } as DocumentPickerResponse\r\n+    }\r\n+    return file\r\n+  }\r\n+\r\n+  const pickLora = async () => {\r\n+    let loraFile\r\n+    const loraRes = await DocumentPicker.pick({\r\n+      type: Platform.OS === 'ios' ? 'public.data' : 'application/octet-stream',\r\n+    }).catch((e) => console.log('No lora file picked, error: ', e.message))\r\n+    if (loraRes?.[0]) loraFile = await copyFileIfNeeded('lora', loraRes[0])\r\n+    return loraFile\r\n+  }\r\n+\r\n+  const handlePickModel = async () => {\r\n+    const modelRes = await DocumentPicker.pick({\r\n+      type: Platform.OS === 'ios' ? 'public.data' : 'application/octet-stream',\r\n+    }).catch((e) => console.log('No model file picked, error: ', e.message))\r\n+    if (!modelRes?.[0]) return\r\n+    const modelFile = await copyFileIfNeeded('model', modelRes?.[0])\r\n+\r\n+    let loraFile: any = null\r\n+    // Example: Apply lora adapter (Currently only select one lora file) (Uncomment to use)\r\n+    // loraFile = await pickLora()\r\n+    loraFile = null\r\n+\r\n+    handleInitContext(modelFile, loraFile)\r\n+  }\r\n+\r\n+  const handleSendPress = async (message: MessageType.PartialText) => {\r\n+    if (context) {\r\n+      switch (message.text) {\r\n+        case '/info':\r\n+          addSystemMessage(\r\n+            `// Model Info\\n${json5.stringify(context.model, null, 2)}`,\r\n+            { copyable: true },\r\n+          )\r\n+          return\r\n+        case '/bench':\r\n+          addSystemMessage('Heating up the model...')\r\n+          const t0 = Date.now()\r\n+          await context.bench(8, 4, 1, 1)\r\n+          const tHeat = Date.now() - t0\r\n+          if (tHeat > 1e4) {\r\n+            addSystemMessage('Heat up time is too long, please try again.')\r\n+            return\r\n+          }\r\n+          addSystemMessage(`Heat up time: ${tHeat}ms`)\r\n+\r\n+          addSystemMessage('Benchmarking the model...')\r\n+          const {\r\n+            modelDesc,\r\n+            modelSize,\r\n+            modelNParams,\r\n+            ppAvg,\r\n+            ppStd,\r\n+            tgAvg,\r\n+            tgStd,\r\n+          } = await context.bench(512, 128, 1, 3)\r\n+\r\n+          const size = `${(modelSize / 1024.0 / 1024.0 / 1024.0).toFixed(\r\n+            2,\r\n+          )} GiB`\r\n+          const nParams = `${(modelNParams / 1e9).toFixed(2)}B`\r\n+          const md =\r\n+            '| model | size | params | test | t/s |\\n' +\r\n+            '| --- | --- | --- | --- | --- |\\n' +\r\n+            `| ${modelDesc} | ${size} | ${nParams} | pp 512 | ${ppAvg.toFixed(\r\n+              2,\r\n+            )} ± ${ppStd.toFixed(2)} |\\n` +\r\n+            `| ${modelDesc} | ${size} | ${nParams} | tg 128 | ${tgAvg.toFixed(\r\n+              2,\r\n+            )} ± ${tgStd.toFixed(2)}`\r\n+          addSystemMessage(md, { copyable: true })\r\n+          return\r\n+        case '/release':\r\n+          await handleReleaseContext()\r\n+          return\r\n+        case '/stop':\r\n+          if (inferencing) context.stopCompletion()\r\n+          return\r\n+        case '/reset':\r\n+          conversationIdRef.current = randId()\r\n+          addSystemMessage('Conversation reset!')\r\n+          return\r\n+        case '/save-session':\r\n+          context\r\n+            .saveSession(`${dirs.DocumentDir}/llama-session.bin`)\r\n+            .then((tokensSaved) => {\r\n+              console.log('Session tokens saved:', tokensSaved)\r\n+              addSystemMessage(`Session saved! ${tokensSaved} tokens saved.`)\r\n+            })\r\n+            .catch((e) => {\r\n+              console.log('Session save failed:', e)\r\n+              addSystemMessage(`Session save failed: ${e.message}`)\r\n+            })\r\n+          return\r\n+        case '/load-session':\r\n+          context\r\n+            .loadSession(`${dirs.DocumentDir}/llama-session.bin`)\r\n+            .then((details) => {\r\n+              console.log('Session loaded:', details)\r\n+              addSystemMessage(\r\n+                `Session loaded! ${details.tokens_loaded} tokens loaded.`,\r\n+              )\r\n+            })\r\n+            .catch((e) => {\r\n+              console.log('Session load failed:', e)\r\n+              addSystemMessage(`Session load failed: ${e.message}`)\r\n+            })\r\n+          return\r\n+        case '/lora':\r\n+          pickLora()\r\n+            .then((loraFile) => {\r\n+              if (loraFile)\r\n+                context.applyLoraAdapters([{ path: loraFile.uri }])\r\n+            })\r\n+            .then(() => context.getLoadedLoraAdapters())\r\n+            .then((loraList) =>\r\n+              addSystemMessage(\r\n+                `Loaded lora adapters: ${JSON.stringify(loraList)}`,\r\n+              ),\r\n+            )\r\n+          return\r\n+        case '/remove-lora':\r\n+          context.removeLoraAdapters().then(() => {\r\n+            addSystemMessage('Lora adapters removed!')\r\n+          })\r\n+          return\r\n+        case '/lora-list':\r\n+          context.getLoadedLoraAdapters().then((loraList) => {\r\n+            addSystemMessage(\r\n+              `Loaded lora adapters: ${JSON.stringify(loraList)}`,\r\n+            )\r\n+          })\r\n+          return\r\n+      }\r\n+    }\r\n+    const textMessage: MessageType.Text = {\r\n+      author: user,\r\n+      createdAt: Date.now(),\r\n+      id: randId(),\r\n+      text: message.text,\r\n+      type: 'text',\r\n+      metadata: {\r\n+        contextId: context?.id,\r\n+        conversationId: conversationIdRef.current,\r\n+      },\r\n+    }\r\n+\r\n+    const id = randId()\r\n+    const createdAt = Date.now()\r\n+    const msgs = [\r\n+      systemMessage,\r\n+      ...[...messages]\r\n+        .reverse()\r\n+        .map((msg) => {\r\n+          if (\r\n+            !msg.metadata?.system &&\r\n+            msg.metadata?.conversationId === conversationIdRef.current &&\r\n+            msg.metadata?.contextId === context?.id &&\r\n+            msg.type === 'text'\r\n+          ) {\r\n+            return {\r\n+              role: msg.author.id === systemId ? 'assistant' : 'user',\r\n+              content: msg.text,\r\n+            }\r\n+          }\r\n+          return { role: '', content: '' }\r\n+        })\r\n+        .filter((msg) => msg.role),\r\n+      { role: 'user', content: message.text },\r\n+    ]\r\n+    addMessage(textMessage)\r\n+    setInferencing(true)\r\n+    // Test area\r\n+    {\r\n+      // Test tokenize\r\n+      const formattedChat = (await context?.getFormattedChat(msgs)) || ''\r\n+      const t0 = Date.now()\r\n+      const { tokens } = (await context?.tokenize(formattedChat)) || {}\r\n+      const t1 = Date.now()\r\n+      console.log(\r\n+        'Formatted:',\r\n+        `\"${formattedChat}\"`,\r\n+        '\\nTokenize:',\r\n+        tokens,\r\n+        `(${tokens?.length} tokens, ${t1 - t0}ms})`,\r\n+      )\r\n+\r\n+      // Test embedding\r\n+      // await context?.embedding(formattedChat).then((result) => {\r\n+      //   console.log('Embedding:', result)\r\n+      // })\r\n+\r\n+      // Test detokenize\r\n+      // await context?.detokenize(tokens).then((result) => {\r\n+      //   console.log('Detokenize:', result)\r\n+      // })\r\n+    }\r\n+\r\n+    let grammar\r\n+    {\r\n+      // Test JSON Schema -> grammar\r\n+      const schema = {\r\n+        oneOf: [\r\n+          {\r\n+            type: 'object',\r\n+            properties: {\r\n+              function: { const: 'create_event' },\r\n+              arguments: {\r\n+                type: 'object',\r\n+                properties: {\r\n+                  title: { type: 'string' },\r\n+                  date: { type: 'string' },\r\n+                  time: { type: 'string' },\r\n+                },\r\n+                required: ['title', 'date'],\r\n+              },\r\n+            },\r\n+            required: ['function', 'arguments'],\r\n+          },\r\n+          {\r\n+            type: 'object',\r\n+            properties: {\r\n+              function: { const: 'image_search' },\r\n+              arguments: {\r\n+                type: 'object',\r\n+                properties: {\r\n+                  query: { type: 'string' },\r\n+                },\r\n+                required: ['query'],\r\n+              },\r\n+            },\r\n+            required: ['function', 'arguments'],\r\n+          },\r\n+        ],\r\n+      }\r\n+\r\n+      const converted = convertJsonSchemaToGrammar({\r\n+        schema,\r\n+        propOrder: { function: 0, arguments: 1 },\r\n+      })\r\n+      // @ts-ignore\r\n+      if (false) console.log('Converted grammar:', converted)\r\n+      grammar = undefined\r\n+      // Uncomment to test:\r\n+      // grammar = converted\r\n+    }\r\n+\r\n+    context\r\n+      ?.completion(\r\n+        {\r\n+          messages: msgs,\r\n+          n_predict: 100,\r\n+          grammar,\r\n+          seed: -1,\r\n+          n_probs: 0,\r\n+\r\n+          // Sampling params\r\n+          top_k: 40,\r\n+          top_p: 0.5,\r\n+          min_p: 0.05,\r\n+          xtc_probability: 0.5,\r\n+          xtc_threshold: 0.1,\r\n+          typical_p: 1.0,\r\n+          temperature: 0.7,\r\n+          penalty_last_n: 64,\r\n+          penalty_repeat: 1.0,\r\n+          penalty_freq: 0.0,\r\n+          penalty_present: 0.0,\r\n+          dry_multiplier: 0,\r\n+          dry_base: 1.75,\r\n+          dry_allowed_length: 2,\r\n+          dry_penalty_last_n: -1,\r\n+          dry_sequence_breakers: ['\\n', ':', '\"', '*'],\r\n+          mirostat: 0,\r\n+          mirostat_tau: 5,\r\n+          mirostat_eta: 0.1,\r\n+          penalize_nl: false,\r\n+          ignore_eos: false,\r\n+          stop: [\r\n+            '\u0007',\r\n+            '<|end|>',\r\n+            '<|eot_id|>',\r\n+            '<|end_of_text|>',\r\n+            '<|im_end|>',\r\n+            '<|EOT|>',\r\n+            '<|END_OF_TURN_TOKEN|>',\r\n+            '<|end_of_turn|>',\r\n+            '<|endoftext|>',\r\n+          ],\r\n+          // n_threads: 4,\r\n+          // logit_bias: [[15043,1.0]],\r\n+        },\r\n+        (data: MessageData) => {\r\n+          const { token } = data\r\n+          setMessages((msgs: Messages) => {\r\n+            const index = msgs.findIndex((msg: Message) => msg.id === id)\r\n+            if (index >= 0) {\r\n+              return msgs.map((msg: Message, i: number) => {\r\n+                if (msg.type === 'text' && i === index) {\r\n+                  return {\r\n+                    ...msg,\r\n+                    text: (msg.text + token).replace(/^\\s+/, ''),\r\n+                  }\r\n+                }\r\n+                return msg\r\n+              })\r\n+            }\r\n+            return [\r\n+              {\r\n+                author: system,\r\n+                createdAt,\r\n+                id,\r\n+                text: token,\r\n+                type: 'text',\r\n+                metadata: {\r\n+                  contextId: context?.id,\r\n+                  conversationId: conversationIdRef.current,\r\n+                },\r\n+              },\r\n+              ...msgs,\r\n+            ]\r\n+          })\r\n+        },\r\n+      )\r\n+      .then((completionResult: CompletionResult) => {\r\n+        console.log('completionResult: ', completionResult)\r\n+        const timings = `${completionResult.timings.predicted_per_token_ms.toFixed()}ms per token, ${completionResult.timings.predicted_per_second.toFixed(\r\n+          2,\r\n+        )} tokens per second`\r\n+        setMessages((msgs: Messages) => {\r\n+          const index = msgs.findIndex((msg: Message) => msg.id === id)\r\n+          if (index >= 0) {\r\n+            return msgs.map((msg: Message, i: number) => {\r\n+              if (msg.type === 'text' && i === index) {\r\n+                return {\r\n+                  ...msg,\r\n+                  metadata: {\r\n+                    ...msg.metadata,\r\n+                    timings,\r\n+                  },\r\n+                }\r\n+              }\r\n+              return msg\r\n+            })\r\n+          }\r\n+          return msgs\r\n+        })\r\n+        setInferencing(false)\r\n+      })\r\n+      .catch((err: LlamaError) => {\r\n+        console.log('completion error: ', err)\r\n+        setInferencing(false)\r\n+        addSystemMessage(`Completion failed: ${err.message}`)\r\n+      })\r\n+  }\r\n+\r\n+  return (\r\n+    <SafeAreaProvider>\r\n+      <Chat\r\n+        renderBubble={renderBubble}\r\n+        theme={darkTheme}\r\n+        messages={messages}\r\n+        onSendPress={handleSendPress}\r\n+        user={user}\r\n+        onAttachmentPress={!context ? handlePickModel : undefined}\r\n+        textInputProps={{\r\n+          editable: !!context,\r\n+          placeholder: !context\r\n+            ? 'Press the file icon to pick a model'\r\n+            : 'Type your message here',\r\n+        }}\r\n+      />\r\n+    </SafeAreaProvider>\r\n+  )\r\n+}\r\n"
                },
                {
                    "date": 1733071409211,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,580 @@\n+/* import React, { useState, useRef } from 'react'\r\n+import type { ReactNode } from 'react'\r\n+import { Platform } from 'react-native'\r\n+import { SafeAreaProvider } from 'react-native-safe-area-context'\r\n+import DocumentPicker from 'react-native-document-picker'\r\n+import type { DocumentPickerResponse } from 'react-native-document-picker'\r\n+import { Chat, darkTheme } from '@flyerhq/react-native-chat-ui'\r\n+import type { MessageType } from '@flyerhq/react-native-chat-ui'\r\n+import json5 from 'json5'\r\n+import ReactNativeBlobUtil from 'react-native-blob-util'\r\n+import type { LlamaContext } from 'llama.rn'\r\n+import {\r\n+  initLlama,\r\n+  loadLlamaModelInfo,\r\n+  convertJsonSchemaToGrammar,\r\n+  // eslint-disable-next-line import/no-unresolved\r\n+} from 'llama.rn'\r\n+import { Bubble } from './Bubble'\r\n+\r\n+const { dirs } = ReactNativeBlobUtil.fs\r\n+\r\n+const randId = () => Math.random().toString(36).substr(2, 9)\r\n+\r\n+const user = { id: 'y9d7f8pgn' }\r\n+\r\n+const systemId = 'h3o3lc5xj'\r\n+const system = { id: systemId }\r\n+\r\n+const systemMessage = {\r\n+  role: 'system',\r\n+  content:\r\n+    'This is a conversation between user and assistant, a friendly chatbot.\\n\\n',\r\n+}\r\n+\r\n+const defaultConversationId = 'default'\r\n+\r\n+const renderBubble = ({\r\n+  child,\r\n+  message,\r\n+}: {\r\n+  child: ReactNode\r\n+  message: MessageType.Any\r\n+}) => <Bubble child={child} message={message} />\r\n+\r\n+// Add type declarations for messages and state\r\n+type Message = MessageType.Any\r\n+type Messages = Message[]\r\n+\r\n+// Add proper types for error handling\r\n+type LlamaError = {\r\n+  message: string\r\n+}\r\n+\r\n+// Add proper types for session and completion results\r\n+type SessionDetails = {\r\n+  tokens_loaded: number\r\n+}\r\n+\r\n+type CompletionResult = {\r\n+  timings: {\r\n+    predicted_per_token_ms: number\r\n+    predicted_per_second: number\r\n+  }\r\n+}\r\n+\r\n+// Add proper types for message handling\r\n+type MessageData = {\r\n+  token: string\r\n+}\r\n+\r\n+export default function App() {\r\n+  const [context, setContext] = useState<LlamaContext | undefined>(undefined)\r\n+\r\n+  const [inferencing, setInferencing] = useState<boolean>(false)\r\n+  const [messages, setMessages] = useState<Message[]>([])\r\n+\r\n+  const conversationIdRef = useRef<string>(defaultConversationId)\r\n+\r\n+  const addMessage = (message: Message, batching = false) => {\r\n+    if (batching) {\r\n+      setMessages([message, ...messages])\r\n+    } else {\r\n+      setMessages((msgs: Messages) => [message, ...msgs])\r\n+    }\r\n+  }\r\n+\r\n+  const addSystemMessage = (text: string, metadata = {}) => {\r\n+    const textMessage: MessageType.Text = {\r\n+      author: system,\r\n+      createdAt: Date.now(),\r\n+      id: randId(),\r\n+      text,\r\n+      type: 'text',\r\n+      metadata: { system: true, ...metadata },\r\n+    }\r\n+    addMessage(textMessage)\r\n+    return textMessage.id\r\n+  }\r\n+\r\n+  const handleReleaseContext = async () => {\r\n+    if (!context) return\r\n+    addSystemMessage('Releasing context...')\r\n+    context\r\n+      .release()\r\n+      .then(() => {\r\n+        setContext(undefined)\r\n+        addSystemMessage('Context released!')\r\n+      })\r\n+      .catch((err: LlamaError) => {\r\n+        addSystemMessage(`Context release failed: ${err.message}`)\r\n+      })\r\n+  }\r\n+\r\n+  // Example: Get model info without initializing context\r\n+  const getModelInfo = async (model: string) => {\r\n+    const t0 = Date.now()\r\n+    const info = await loadLlamaModelInfo(model)\r\n+    console.log(`Model info (took ${Date.now() - t0}ms): `, info)\r\n+  }\r\n+\r\n+  const handleInitContext = async (\r\n+    file: DocumentPickerResponse,\r\n+    loraFile: DocumentPickerResponse | null,\r\n+  ) => {\r\n+    await handleReleaseContext()\r\n+    await getModelInfo(file.uri)\r\n+    const msgId = addSystemMessage('Initializing context...')\r\n+    const t0 = Date.now()\r\n+    try {\r\n+      const ctx = await initLlama(\r\n+        {\r\n+          model: file.uri,\r\n+          use_mlock: true,\r\n+          n_gpu_layers: Platform.OS === 'ios' ? 99 : 0,\r\n+          use_progress_callback: true,\r\n+          lora_list: loraFile ? [{ path: loraFile.uri, scaled: 1.0 }] : undefined,\r\n+        },\r\n+        (progress: number) => {\r\n+          setMessages((msgs: Messages) => {\r\n+            const index = msgs.findIndex((msg: Message) => msg.id === msgId)\r\n+            if (index >= 0) {\r\n+              return msgs.map((msg: Message, i: number) => {\r\n+                if (msg.type === 'text' && i === index) {\r\n+                  return {\r\n+                    ...msg,\r\n+                    text: `Initializing context... ${progress}%`,\r\n+                  }\r\n+                }\r\n+                return msg\r\n+              })\r\n+            }\r\n+            return msgs\r\n+          })\r\n+        },\r\n+      )\r\n+      const t1 = Date.now()\r\n+      setContext(ctx)\r\n+      addSystemMessage(\r\n+        `Context initialized!\\n\\nLoad time: ${t1 - t0}ms\\nGPU: ${\r\n+          ctx.gpu ? 'YES' : 'NO'\r\n+        } (${ctx.reasonNoGPU})\\nChat Template: ${\r\n+          ctx.model.isChatTemplateSupported ? 'YES' : 'NO'\r\n+        }\\n\\n` +\r\n+          'You can use the following commands:\\n\\n' +\r\n+          '- /info: to get the model info\\n' +\r\n+          '- /bench: to benchmark the model\\n' +\r\n+          '- /release: release the context\\n' +\r\n+          '- /stop: stop the current completion\\n' +\r\n+          '- /reset: reset the conversation' +\r\n+          '- /save-session: save the session tokens\\n' +\r\n+          '- /load-session: load the session tokens',\r\n+      )\r\n+    } catch (err: unknown) {\r\n+      const error = err as LlamaError\r\n+      const errorMsg = error.message || 'Unknown error'\r\n+      addSystemMessage(`Context initialization failed: ${errorMsg}\\n\\nPlease check:\\n1. Model file exists and is accessible\\n2. Sufficient memory available\\n3. Model file is not corrupted`)\r\n+    }\r\n+  }\r\n+\r\n+  const copyFileIfNeeded = async (\r\n+    type = 'model',\r\n+    file: DocumentPickerResponse,\r\n+  ) => {\r\n+    if (Platform.OS === 'android' && file.uri.startsWith('content://')) {\r\n+      const dir = `${ReactNativeBlobUtil.fs.dirs.CacheDir}/${type}s`\r\n+      const filepath = `${dir}/${file.uri.split('/').pop() || type}.gguf`\r\n+\r\n+      if (!(await ReactNativeBlobUtil.fs.isDir(dir)))\r\n+        await ReactNativeBlobUtil.fs.mkdir(dir)\r\n+\r\n+      if (await ReactNativeBlobUtil.fs.exists(filepath))\r\n+        return { uri: filepath } as DocumentPickerResponse\r\n+\r\n+      await ReactNativeBlobUtil.fs.unlink(dir) // Clean up old files in models\r\n+\r\n+      addSystemMessage(`Copying ${type} to internal storage...`)\r\n+      await ReactNativeBlobUtil.MediaCollection.copyToInternal(\r\n+        file.uri,\r\n+        filepath,\r\n+      )\r\n+      addSystemMessage(`${type} copied!`)\r\n+      return { uri: filepath } as DocumentPickerResponse\r\n+    }\r\n+    return file\r\n+  }\r\n+\r\n+  const pickLora = async () => {\r\n+    let loraFile\r\n+    const loraRes = await DocumentPicker.pick({\r\n+      type: Platform.OS === 'ios' ? 'public.data' : 'application/octet-stream',\r\n+    }).catch((e) => console.log('No lora file picked, error: ', e.message))\r\n+    if (loraRes?.[0]) loraFile = await copyFileIfNeeded('lora', loraRes[0])\r\n+    return loraFile\r\n+  }\r\n+\r\n+  const handlePickModel = async () => {\r\n+    const modelRes = await DocumentPicker.pick({\r\n+      type: Platform.OS === 'ios' ? 'public.data' : 'application/octet-stream',\r\n+    }).catch((e) => console.log('No model file picked, error: ', e.message))\r\n+    if (!modelRes?.[0]) return\r\n+    const modelFile = await copyFileIfNeeded('model', modelRes?.[0])\r\n+\r\n+    let loraFile: any = null\r\n+    // Example: Apply lora adapter (Currently only select one lora file) (Uncomment to use)\r\n+    // loraFile = await pickLora()\r\n+    loraFile = null\r\n+\r\n+    handleInitContext(modelFile, loraFile)\r\n+  }\r\n+\r\n+  const handleSendPress = async (message: MessageType.PartialText) => {\r\n+    if (context) {\r\n+      switch (message.text) {\r\n+        case '/info':\r\n+          addSystemMessage(\r\n+            `// Model Info\\n${json5.stringify(context.model, null, 2)}`,\r\n+            { copyable: true },\r\n+          )\r\n+          return\r\n+        case '/bench':\r\n+          addSystemMessage('Heating up the model...')\r\n+          const t0 = Date.now()\r\n+          await context.bench(8, 4, 1, 1)\r\n+          const tHeat = Date.now() - t0\r\n+          if (tHeat > 1e4) {\r\n+            addSystemMessage('Heat up time is too long, please try again.')\r\n+            return\r\n+          }\r\n+          addSystemMessage(`Heat up time: ${tHeat}ms`)\r\n+\r\n+          addSystemMessage('Benchmarking the model...')\r\n+          const {\r\n+            modelDesc,\r\n+            modelSize,\r\n+            modelNParams,\r\n+            ppAvg,\r\n+            ppStd,\r\n+            tgAvg,\r\n+            tgStd,\r\n+          } = await context.bench(512, 128, 1, 3)\r\n+\r\n+          const size = `${(modelSize / 1024.0 / 1024.0 / 1024.0).toFixed(\r\n+            2,\r\n+          )} GiB`\r\n+          const nParams = `${(modelNParams / 1e9).toFixed(2)}B`\r\n+          const md =\r\n+            '| model | size | params | test | t/s |\\n' +\r\n+            '| --- | --- | --- | --- | --- |\\n' +\r\n+            `| ${modelDesc} | ${size} | ${nParams} | pp 512 | ${ppAvg.toFixed(\r\n+              2,\r\n+            )} ± ${ppStd.toFixed(2)} |\\n` +\r\n+            `| ${modelDesc} | ${size} | ${nParams} | tg 128 | ${tgAvg.toFixed(\r\n+              2,\r\n+            )} ± ${tgStd.toFixed(2)}`\r\n+          addSystemMessage(md, { copyable: true })\r\n+          return\r\n+        case '/release':\r\n+          await handleReleaseContext()\r\n+          return\r\n+        case '/stop':\r\n+          if (inferencing) context.stopCompletion()\r\n+          return\r\n+        case '/reset':\r\n+          conversationIdRef.current = randId()\r\n+          addSystemMessage('Conversation reset!')\r\n+          return\r\n+        case '/save-session':\r\n+          context\r\n+            .saveSession(`${dirs.DocumentDir}/llama-session.bin`)\r\n+            .then((tokensSaved) => {\r\n+              console.log('Session tokens saved:', tokensSaved)\r\n+              addSystemMessage(`Session saved! ${tokensSaved} tokens saved.`)\r\n+            })\r\n+            .catch((e) => {\r\n+              console.log('Session save failed:', e)\r\n+              addSystemMessage(`Session save failed: ${e.message}`)\r\n+            })\r\n+          return\r\n+        case '/load-session':\r\n+          context\r\n+            .loadSession(`${dirs.DocumentDir}/llama-session.bin`)\r\n+            .then((details) => {\r\n+              console.log('Session loaded:', details)\r\n+              addSystemMessage(\r\n+                `Session loaded! ${details.tokens_loaded} tokens loaded.`,\r\n+              )\r\n+            })\r\n+            .catch((e) => {\r\n+              console.log('Session load failed:', e)\r\n+              addSystemMessage(`Session load failed: ${e.message}`)\r\n+            })\r\n+          return\r\n+        case '/lora':\r\n+          pickLora()\r\n+            .then((loraFile) => {\r\n+              if (loraFile)\r\n+                context.applyLoraAdapters([{ path: loraFile.uri }])\r\n+            })\r\n+            .then(() => context.getLoadedLoraAdapters())\r\n+            .then((loraList) =>\r\n+              addSystemMessage(\r\n+                `Loaded lora adapters: ${JSON.stringify(loraList)}`,\r\n+              ),\r\n+            )\r\n+          return\r\n+        case '/remove-lora':\r\n+          context.removeLoraAdapters().then(() => {\r\n+            addSystemMessage('Lora adapters removed!')\r\n+          })\r\n+          return\r\n+        case '/lora-list':\r\n+          context.getLoadedLoraAdapters().then((loraList) => {\r\n+            addSystemMessage(\r\n+              `Loaded lora adapters: ${JSON.stringify(loraList)}`,\r\n+            )\r\n+          })\r\n+          return\r\n+      }\r\n+    }\r\n+    const textMessage: MessageType.Text = {\r\n+      author: user,\r\n+      createdAt: Date.now(),\r\n+      id: randId(),\r\n+      text: message.text,\r\n+      type: 'text',\r\n+      metadata: {\r\n+        contextId: context?.id,\r\n+        conversationId: conversationIdRef.current,\r\n+      },\r\n+    }\r\n+\r\n+    const id = randId()\r\n+    const createdAt = Date.now()\r\n+    const msgs = [\r\n+      systemMessage,\r\n+      ...[...messages]\r\n+        .reverse()\r\n+        .map((msg) => {\r\n+          if (\r\n+            !msg.metadata?.system &&\r\n+            msg.metadata?.conversationId === conversationIdRef.current &&\r\n+            msg.metadata?.contextId === context?.id &&\r\n+            msg.type === 'text'\r\n+          ) {\r\n+            return {\r\n+              role: msg.author.id === systemId ? 'assistant' : 'user',\r\n+              content: msg.text,\r\n+            }\r\n+          }\r\n+          return { role: '', content: '' }\r\n+        })\r\n+        .filter((msg) => msg.role),\r\n+      { role: 'user', content: message.text },\r\n+    ]\r\n+    addMessage(textMessage)\r\n+    setInferencing(true)\r\n+    // Test area\r\n+    {\r\n+      // Test tokenize\r\n+      const formattedChat = (await context?.getFormattedChat(msgs)) || ''\r\n+      const t0 = Date.now()\r\n+      const { tokens } = (await context?.tokenize(formattedChat)) || {}\r\n+      const t1 = Date.now()\r\n+      console.log(\r\n+        'Formatted:',\r\n+        `\"${formattedChat}\"`,\r\n+        '\\nTokenize:',\r\n+        tokens,\r\n+        `(${tokens?.length} tokens, ${t1 - t0}ms})`,\r\n+      )\r\n+\r\n+      // Test embedding\r\n+      // await context?.embedding(formattedChat).then((result) => {\r\n+      //   console.log('Embedding:', result)\r\n+      // })\r\n+\r\n+      // Test detokenize\r\n+      // await context?.detokenize(tokens).then((result) => {\r\n+      //   console.log('Detokenize:', result)\r\n+      // })\r\n+    }\r\n+\r\n+    let grammar\r\n+    {\r\n+      // Test JSON Schema -> grammar\r\n+      const schema = {\r\n+        oneOf: [\r\n+          {\r\n+            type: 'object',\r\n+            properties: {\r\n+              function: { const: 'create_event' },\r\n+              arguments: {\r\n+                type: 'object',\r\n+                properties: {\r\n+                  title: { type: 'string' },\r\n+                  date: { type: 'string' },\r\n+                  time: { type: 'string' },\r\n+                },\r\n+                required: ['title', 'date'],\r\n+              },\r\n+            },\r\n+            required: ['function', 'arguments'],\r\n+          },\r\n+          {\r\n+            type: 'object',\r\n+            properties: {\r\n+              function: { const: 'image_search' },\r\n+              arguments: {\r\n+                type: 'object',\r\n+                properties: {\r\n+                  query: { type: 'string' },\r\n+                },\r\n+                required: ['query'],\r\n+              },\r\n+            },\r\n+            required: ['function', 'arguments'],\r\n+          },\r\n+        ],\r\n+      }\r\n+\r\n+      const converted = convertJsonSchemaToGrammar({\r\n+        schema,\r\n+        propOrder: { function: 0, arguments: 1 },\r\n+      })\r\n+      // @ts-ignore\r\n+      if (false) console.log('Converted grammar:', converted)\r\n+      grammar = undefined\r\n+      // Uncomment to test:\r\n+      // grammar = converted\r\n+    }\r\n+\r\n+    context\r\n+      ?.completion(\r\n+        {\r\n+          messages: msgs,\r\n+          n_predict: 100,\r\n+          grammar,\r\n+          seed: -1,\r\n+          n_probs: 0,\r\n+\r\n+          // Sampling params\r\n+          top_k: 40,\r\n+          top_p: 0.5,\r\n+          min_p: 0.05,\r\n+          xtc_probability: 0.5,\r\n+          xtc_threshold: 0.1,\r\n+          typical_p: 1.0,\r\n+          temperature: 0.7,\r\n+          penalty_last_n: 64,\r\n+          penalty_repeat: 1.0,\r\n+          penalty_freq: 0.0,\r\n+          penalty_present: 0.0,\r\n+          dry_multiplier: 0,\r\n+          dry_base: 1.75,\r\n+          dry_allowed_length: 2,\r\n+          dry_penalty_last_n: -1,\r\n+          dry_sequence_breakers: ['\\n', ':', '\"', '*'],\r\n+          mirostat: 0,\r\n+          mirostat_tau: 5,\r\n+          mirostat_eta: 0.1,\r\n+          penalize_nl: false,\r\n+          ignore_eos: false,\r\n+          stop: [\r\n+            '\u0007',\r\n+            '<|end|>',\r\n+            '<|eot_id|>',\r\n+            '<|end_of_text|>',\r\n+            '<|im_end|>',\r\n+            '<|EOT|>',\r\n+            '<|END_OF_TURN_TOKEN|>',\r\n+            '<|end_of_turn|>',\r\n+            '<|endoftext|>',\r\n+          ],\r\n+          // n_threads: 4,\r\n+          // logit_bias: [[15043,1.0]],\r\n+        },\r\n+        (data: MessageData) => {\r\n+          const { token } = data\r\n+          setMessages((msgs: Messages) => {\r\n+            const index = msgs.findIndex((msg: Message) => msg.id === id)\r\n+            if (index >= 0) {\r\n+              return msgs.map((msg: Message, i: number) => {\r\n+                if (msg.type === 'text' && i === index) {\r\n+                  return {\r\n+                    ...msg,\r\n+                    text: (msg.text + token).replace(/^\\s+/, ''),\r\n+                  }\r\n+                }\r\n+                return msg\r\n+              })\r\n+            }\r\n+            return [\r\n+              {\r\n+                author: system,\r\n+                createdAt,\r\n+                id,\r\n+                text: token,\r\n+                type: 'text',\r\n+                metadata: {\r\n+                  contextId: context?.id,\r\n+                  conversationId: conversationIdRef.current,\r\n+                },\r\n+              },\r\n+              ...msgs,\r\n+            ]\r\n+          })\r\n+        },\r\n+      )\r\n+      .then((completionResult: CompletionResult) => {\r\n+        console.log('completionResult: ', completionResult)\r\n+        const timings = `${completionResult.timings.predicted_per_token_ms.toFixed()}ms per token, ${completionResult.timings.predicted_per_second.toFixed(\r\n+          2,\r\n+        )} tokens per second`\r\n+        setMessages((msgs: Messages) => {\r\n+          const index = msgs.findIndex((msg: Message) => msg.id === id)\r\n+          if (index >= 0) {\r\n+            return msgs.map((msg: Message, i: number) => {\r\n+              if (msg.type === 'text' && i === index) {\r\n+                return {\r\n+                  ...msg,\r\n+                  metadata: {\r\n+                    ...msg.metadata,\r\n+                    timings,\r\n+                  },\r\n+                }\r\n+              }\r\n+              return msg\r\n+            })\r\n+          }\r\n+          return msgs\r\n+        })\r\n+        setInferencing(false)\r\n+      })\r\n+      .catch((err: LlamaError) => {\r\n+        console.log('completion error: ', err)\r\n+        setInferencing(false)\r\n+        addSystemMessage(`Completion failed: ${err.message}`)\r\n+      })\r\n+  }\r\n+\r\n+  return (\r\n+    <SafeAreaProvider>\r\n+      <Chat\r\n+        renderBubble={renderBubble}\r\n+        theme={darkTheme}\r\n+        messages={messages}\r\n+        onSendPress={handleSendPress}\r\n+        user={user}\r\n+        onAttachmentPress={!context ? handlePickModel : undefined}\r\n+        textInputProps={{\r\n+          editable: !!context,\r\n+          placeholder: !context\r\n+            ? 'Press the file icon to pick a model'\r\n+            : 'Type your message here',\r\n+        }}\r\n+      />\r\n+    </SafeAreaProvider>\r\n+  )\r\n+}\r\n+ */\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733070571471,
            "name": "Commit-0",
            "content": "import React, { useState, useRef } from 'react'\r\nimport type { ReactNode } from 'react'\r\nimport { Platform } from 'react-native'\r\nimport { SafeAreaProvider } from 'react-native-safe-area-context'\r\nimport DocumentPicker from 'react-native-document-picker'\r\nimport type { DocumentPickerResponse } from 'react-native-document-picker'\r\nimport { Chat, darkTheme } from '@flyerhq/react-native-chat-ui'\r\nimport type { MessageType } from '@flyerhq/react-native-chat-ui'\r\nimport json5 from 'json5'\r\nimport ReactNativeBlobUtil from 'react-native-blob-util'\r\nimport type { LlamaContext } from 'llama.rn'\r\nimport {\r\n  initLlama,\r\n  loadLlamaModelInfo,\r\n  convertJsonSchemaToGrammar,\r\n  // eslint-disable-next-line import/no-unresolved\r\n} from 'llama.rn'\r\nimport { Bubble } from './Bubble'\r\n\r\nconst { dirs } = ReactNativeBlobUtil.fs\r\n\r\nconst randId = () => Math.random().toString(36).substr(2, 9)\r\n\r\nconst user = { id: 'y9d7f8pgn' }\r\n\r\nconst systemId = 'h3o3lc5xj'\r\nconst system = { id: systemId }\r\n\r\nconst systemMessage = {\r\n  role: 'system',\r\n  content:\r\n    'This is a conversation between user and assistant, a friendly chatbot.\\n\\n',\r\n}\r\n\r\nconst defaultConversationId = 'default'\r\n\r\nconst renderBubble = ({\r\n  child,\r\n  message,\r\n}: {\r\n  child: ReactNode\r\n  message: MessageType.Any\r\n}) => <Bubble child={child} message={message} />\r\n\r\n// Add type declarations for messages and state\r\ntype Message = MessageType.Any\r\ntype Messages = Message[]\r\n\r\n// Add proper types for error handling\r\ntype LlamaError = {\r\n  message: string\r\n}\r\n\r\n// Add proper types for session and completion results\r\ntype SessionDetails = {\r\n  tokens_loaded: number\r\n}\r\n\r\ntype CompletionResult = {\r\n  timings: {\r\n    predicted_per_token_ms: number\r\n    predicted_per_second: number\r\n  }\r\n}\r\n\r\n// Add proper types for message handling\r\ntype MessageData = {\r\n  token: string\r\n}\r\n\r\nexport default function App() {\r\n  const [context, setContext] = useState<LlamaContext | undefined>(undefined)\r\n\r\n  const [inferencing, setInferencing] = useState<boolean>(false)\r\n  const [messages, setMessages] = useState<Message[]>([])\r\n\r\n  const conversationIdRef = useRef<string>(defaultConversationId)\r\n\r\n  const addMessage = (message: Message, batching = false) => {\r\n    if (batching) {\r\n      setMessages([message, ...messages])\r\n    } else {\r\n      setMessages((msgs: Messages) => [message, ...msgs])\r\n    }\r\n  }\r\n\r\n  const addSystemMessage = (text: string, metadata = {}) => {\r\n    const textMessage: MessageType.Text = {\r\n      author: system,\r\n      createdAt: Date.now(),\r\n      id: randId(),\r\n      text,\r\n      type: 'text',\r\n      metadata: { system: true, ...metadata },\r\n    }\r\n    addMessage(textMessage)\r\n    return textMessage.id\r\n  }\r\n\r\n  const handleReleaseContext = async () => {\r\n    if (!context) return\r\n    addSystemMessage('Releasing context...')\r\n    context\r\n      .release()\r\n      .then(() => {\r\n        setContext(undefined)\r\n        addSystemMessage('Context released!')\r\n      })\r\n      .catch((err: LlamaError) => {\r\n        addSystemMessage(`Context release failed: ${err.message}`)\r\n      })\r\n  }\r\n\r\n  // Example: Get model info without initializing context\r\n  const getModelInfo = async (model: string) => {\r\n    const t0 = Date.now()\r\n    const info = await loadLlamaModelInfo(model)\r\n    console.log(`Model info (took ${Date.now() - t0}ms): `, info)\r\n  }\r\n\r\n  const handleInitContext = async (\r\n    file: DocumentPickerResponse,\r\n    loraFile: DocumentPickerResponse | null,\r\n  ) => {\r\n    await handleReleaseContext()\r\n    await getModelInfo(file.uri)\r\n    const msgId = addSystemMessage('Initializing context...')\r\n    const t0 = Date.now()\r\n    try {\r\n      const ctx = await initLlama(\r\n        {\r\n          model: file.uri,\r\n          use_mlock: true,\r\n          n_gpu_layers: Platform.OS === 'ios' ? 99 : 0,\r\n          use_progress_callback: true,\r\n          lora_list: loraFile ? [{ path: loraFile.uri, scaled: 1.0 }] : undefined,\r\n        },\r\n        (progress: number) => {\r\n          setMessages((msgs: Messages) => {\r\n            const index = msgs.findIndex((msg: Message) => msg.id === msgId)\r\n            if (index >= 0) {\r\n              return msgs.map((msg: Message, i: number) => {\r\n                if (msg.type === 'text' && i === index) {\r\n                  return {\r\n                    ...msg,\r\n                    text: `Initializing context... ${progress}%`,\r\n                  }\r\n                }\r\n                return msg\r\n              })\r\n            }\r\n            return msgs\r\n          })\r\n        },\r\n      )\r\n      const t1 = Date.now()\r\n      setContext(ctx)\r\n      addSystemMessage(\r\n        `Context initialized!\\n\\nLoad time: ${t1 - t0}ms\\nGPU: ${\r\n          ctx.gpu ? 'YES' : 'NO'\r\n        } (${ctx.reasonNoGPU})\\nChat Template: ${\r\n          ctx.model.isChatTemplateSupported ? 'YES' : 'NO'\r\n        }\\n\\n` +\r\n          'You can use the following commands:\\n\\n' +\r\n          '- /info: to get the model info\\n' +\r\n          '- /bench: to benchmark the model\\n' +\r\n          '- /release: release the context\\n' +\r\n          '- /stop: stop the current completion\\n' +\r\n          '- /reset: reset the conversation' +\r\n          '- /save-session: save the session tokens\\n' +\r\n          '- /load-session: load the session tokens',\r\n      )\r\n    } catch (err: unknown) {\r\n      const error = err as LlamaError\r\n      const errorMsg = error.message || 'Unknown error'\r\n      addSystemMessage(`Context initialization failed: ${errorMsg}\\n\\nPlease check:\\n1. Model file exists and is accessible\\n2. Sufficient memory available\\n3. Model file is not corrupted`)\r\n    }\r\n  }\r\n\r\n  const copyFileIfNeeded = async (\r\n    type = 'model',\r\n    file: DocumentPickerResponse,\r\n  ) => {\r\n    if (Platform.OS === 'android' && file.uri.startsWith('content://')) {\r\n      const dir = `${ReactNativeBlobUtil.fs.dirs.CacheDir}/${type}s`\r\n      const filepath = `${dir}/${file.uri.split('/').pop() || type}.gguf`\r\n\r\n      if (!(await ReactNativeBlobUtil.fs.isDir(dir)))\r\n        await ReactNativeBlobUtil.fs.mkdir(dir)\r\n\r\n      if (await ReactNativeBlobUtil.fs.exists(filepath))\r\n        return { uri: filepath } as DocumentPickerResponse\r\n\r\n      await ReactNativeBlobUtil.fs.unlink(dir) // Clean up old files in models\r\n\r\n      addSystemMessage(`Copying ${type} to internal storage...`)\r\n      await ReactNativeBlobUtil.MediaCollection.copyToInternal(\r\n        file.uri,\r\n        filepath,\r\n      )\r\n      addSystemMessage(`${type} copied!`)\r\n      return { uri: filepath } as DocumentPickerResponse\r\n    }\r\n    return file\r\n  }\r\n\r\n  const pickLora = async () => {\r\n    let loraFile\r\n    const loraRes = await DocumentPicker.pick({\r\n      type: Platform.OS === 'ios' ? 'public.data' : 'application/octet-stream',\r\n    }).catch((e) => console.log('No lora file picked, error: ', e.message))\r\n    if (loraRes?.[0]) loraFile = await copyFileIfNeeded('lora', loraRes[0])\r\n    return loraFile\r\n  }\r\n\r\n  const handlePickModel = async () => {\r\n    const modelRes = await DocumentPicker.pick({\r\n      type: Platform.OS === 'ios' ? 'public.data' : 'application/octet-stream',\r\n    }).catch((e) => console.log('No model file picked, error: ', e.message))\r\n    if (!modelRes?.[0]) return\r\n    const modelFile = await copyFileIfNeeded('model', modelRes?.[0])\r\n\r\n    let loraFile: any = null\r\n    // Example: Apply lora adapter (Currently only select one lora file) (Uncomment to use)\r\n    // loraFile = await pickLora()\r\n    loraFile = null\r\n\r\n    handleInitContext(modelFile, loraFile)\r\n  }\r\n\r\n  const handleSendPress = async (message: MessageType.PartialText) => {\r\n    if (context) {\r\n      switch (message.text) {\r\n        case '/info':\r\n          addSystemMessage(\r\n            `// Model Info\\n${json5.stringify(context.model, null, 2)}`,\r\n            { copyable: true },\r\n          )\r\n          return\r\n        case '/bench':\r\n          addSystemMessage('Heating up the model...')\r\n          const t0 = Date.now()\r\n          await context.bench(8, 4, 1, 1)\r\n          const tHeat = Date.now() - t0\r\n          if (tHeat > 1e4) {\r\n            addSystemMessage('Heat up time is too long, please try again.')\r\n            return\r\n          }\r\n          addSystemMessage(`Heat up time: ${tHeat}ms`)\r\n\r\n          addSystemMessage('Benchmarking the model...')\r\n          const {\r\n            modelDesc,\r\n            modelSize,\r\n            modelNParams,\r\n            ppAvg,\r\n            ppStd,\r\n            tgAvg,\r\n            tgStd,\r\n          } = await context.bench(512, 128, 1, 3)\r\n\r\n          const size = `${(modelSize / 1024.0 / 1024.0 / 1024.0).toFixed(\r\n            2,\r\n          )} GiB`\r\n          const nParams = `${(modelNParams / 1e9).toFixed(2)}B`\r\n          const md =\r\n            '| model | size | params | test | t/s |\\n' +\r\n            '| --- | --- | --- | --- | --- |\\n' +\r\n            `| ${modelDesc} | ${size} | ${nParams} | pp 512 | ${ppAvg.toFixed(\r\n              2,\r\n            )} ± ${ppStd.toFixed(2)} |\\n` +\r\n            `| ${modelDesc} | ${size} | ${nParams} | tg 128 | ${tgAvg.toFixed(\r\n              2,\r\n            )} ± ${tgStd.toFixed(2)}`\r\n          addSystemMessage(md, { copyable: true })\r\n          return\r\n        case '/release':\r\n          await handleReleaseContext()\r\n          return\r\n        case '/stop':\r\n          if (inferencing) context.stopCompletion()\r\n          return\r\n        case '/reset':\r\n          conversationIdRef.current = randId()\r\n          addSystemMessage('Conversation reset!')\r\n          return\r\n        case '/save-session':\r\n          context\r\n            .saveSession(`${dirs.DocumentDir}/llama-session.bin`)\r\n            .then((tokensSaved) => {\r\n              console.log('Session tokens saved:', tokensSaved)\r\n              addSystemMessage(`Session saved! ${tokensSaved} tokens saved.`)\r\n            })\r\n            .catch((e) => {\r\n              console.log('Session save failed:', e)\r\n              addSystemMessage(`Session save failed: ${e.message}`)\r\n            })\r\n          return\r\n        case '/load-session':\r\n          context\r\n            .loadSession(`${dirs.DocumentDir}/llama-session.bin`)\r\n            .then((details) => {\r\n              console.log('Session loaded:', details)\r\n              addSystemMessage(\r\n                `Session loaded! ${details.tokens_loaded} tokens loaded.`,\r\n              )\r\n            })\r\n            .catch((e) => {\r\n              console.log('Session load failed:', e)\r\n              addSystemMessage(`Session load failed: ${e.message}`)\r\n            })\r\n          return\r\n        case '/lora':\r\n          pickLora()\r\n            .then((loraFile) => {\r\n              if (loraFile)\r\n                context.applyLoraAdapters([{ path: loraFile.uri }])\r\n            })\r\n            .then(() => context.getLoadedLoraAdapters())\r\n            .then((loraList) =>\r\n              addSystemMessage(\r\n                `Loaded lora adapters: ${JSON.stringify(loraList)}`,\r\n              ),\r\n            )\r\n          return\r\n        case '/remove-lora':\r\n          context.removeLoraAdapters().then(() => {\r\n            addSystemMessage('Lora adapters removed!')\r\n          })\r\n          return\r\n        case '/lora-list':\r\n          context.getLoadedLoraAdapters().then((loraList) => {\r\n            addSystemMessage(\r\n              `Loaded lora adapters: ${JSON.stringify(loraList)}`,\r\n            )\r\n          })\r\n          return\r\n      }\r\n    }\r\n    const textMessage: MessageType.Text = {\r\n      author: user,\r\n      createdAt: Date.now(),\r\n      id: randId(),\r\n      text: message.text,\r\n      type: 'text',\r\n      metadata: {\r\n        contextId: context?.id,\r\n        conversationId: conversationIdRef.current,\r\n      },\r\n    }\r\n\r\n    const id = randId()\r\n    const createdAt = Date.now()\r\n    const msgs = [\r\n      systemMessage,\r\n      ...[...messages]\r\n        .reverse()\r\n        .map((msg) => {\r\n          if (\r\n            !msg.metadata?.system &&\r\n            msg.metadata?.conversationId === conversationIdRef.current &&\r\n            msg.metadata?.contextId === context?.id &&\r\n            msg.type === 'text'\r\n          ) {\r\n            return {\r\n              role: msg.author.id === systemId ? 'assistant' : 'user',\r\n              content: msg.text,\r\n            }\r\n          }\r\n          return { role: '', content: '' }\r\n        })\r\n        .filter((msg) => msg.role),\r\n      { role: 'user', content: message.text },\r\n    ]\r\n    addMessage(textMessage)\r\n    setInferencing(true)\r\n    // Test area\r\n    {\r\n      // Test tokenize\r\n      const formattedChat = (await context?.getFormattedChat(msgs)) || ''\r\n      const t0 = Date.now()\r\n      const { tokens } = (await context?.tokenize(formattedChat)) || {}\r\n      const t1 = Date.now()\r\n      console.log(\r\n        'Formatted:',\r\n        `\"${formattedChat}\"`,\r\n        '\\nTokenize:',\r\n        tokens,\r\n        `(${tokens?.length} tokens, ${t1 - t0}ms})`,\r\n      )\r\n\r\n      // Test embedding\r\n      // await context?.embedding(formattedChat).then((result) => {\r\n      //   console.log('Embedding:', result)\r\n      // })\r\n\r\n      // Test detokenize\r\n      // await context?.detokenize(tokens).then((result) => {\r\n      //   console.log('Detokenize:', result)\r\n      // })\r\n    }\r\n\r\n    let grammar\r\n    {\r\n      // Test JSON Schema -> grammar\r\n      const schema = {\r\n        oneOf: [\r\n          {\r\n            type: 'object',\r\n            properties: {\r\n              function: { const: 'create_event' },\r\n              arguments: {\r\n                type: 'object',\r\n                properties: {\r\n                  title: { type: 'string' },\r\n                  date: { type: 'string' },\r\n                  time: { type: 'string' },\r\n                },\r\n                required: ['title', 'date'],\r\n              },\r\n            },\r\n            required: ['function', 'arguments'],\r\n          },\r\n          {\r\n            type: 'object',\r\n            properties: {\r\n              function: { const: 'image_search' },\r\n              arguments: {\r\n                type: 'object',\r\n                properties: {\r\n                  query: { type: 'string' },\r\n                },\r\n                required: ['query'],\r\n              },\r\n            },\r\n            required: ['function', 'arguments'],\r\n          },\r\n        ],\r\n      }\r\n\r\n      const converted = convertJsonSchemaToGrammar({\r\n        schema,\r\n        propOrder: { function: 0, arguments: 1 },\r\n      })\r\n      // @ts-ignore\r\n      if (false) console.log('Converted grammar:', converted)\r\n      grammar = undefined\r\n      // Uncomment to test:\r\n      // grammar = converted\r\n    }\r\n\r\n    context\r\n      ?.completion(\r\n        {\r\n          messages: msgs,\r\n          n_predict: 100,\r\n          grammar,\r\n          seed: -1,\r\n          n_probs: 0,\r\n\r\n          // Sampling params\r\n          top_k: 40,\r\n          top_p: 0.5,\r\n          min_p: 0.05,\r\n          xtc_probability: 0.5,\r\n          xtc_threshold: 0.1,\r\n          typical_p: 1.0,\r\n          temperature: 0.7,\r\n          penalty_last_n: 64,\r\n          penalty_repeat: 1.0,\r\n          penalty_freq: 0.0,\r\n          penalty_present: 0.0,\r\n          dry_multiplier: 0,\r\n          dry_base: 1.75,\r\n          dry_allowed_length: 2,\r\n          dry_penalty_last_n: -1,\r\n          dry_sequence_breakers: ['\\n', ':', '\"', '*'],\r\n          mirostat: 0,\r\n          mirostat_tau: 5,\r\n          mirostat_eta: 0.1,\r\n          penalize_nl: false,\r\n          ignore_eos: false,\r\n          stop: [\r\n            '\u0007',\r\n            '<|end|>',\r\n            '<|eot_id|>',\r\n            '<|end_of_text|>',\r\n            '<|im_end|>',\r\n            '<|EOT|>',\r\n            '<|END_OF_TURN_TOKEN|>',\r\n            '<|end_of_turn|>',\r\n            '<|endoftext|>',\r\n          ],\r\n          // n_threads: 4,\r\n          // logit_bias: [[15043,1.0]],\r\n        },\r\n        (data: MessageData) => {\r\n          const { token } = data\r\n          setMessages((msgs: Messages) => {\r\n            const index = msgs.findIndex((msg: Message) => msg.id === id)\r\n            if (index >= 0) {\r\n              return msgs.map((msg: Message, i: number) => {\r\n                if (msg.type === 'text' && i === index) {\r\n                  return {\r\n                    ...msg,\r\n                    text: (msg.text + token).replace(/^\\s+/, ''),\r\n                  }\r\n                }\r\n                return msg\r\n              })\r\n            }\r\n            return [\r\n              {\r\n                author: system,\r\n                createdAt,\r\n                id,\r\n                text: token,\r\n                type: 'text',\r\n                metadata: {\r\n                  contextId: context?.id,\r\n                  conversationId: conversationIdRef.current,\r\n                },\r\n              },\r\n              ...msgs,\r\n            ]\r\n          })\r\n        },\r\n      )\r\n      .then((completionResult: CompletionResult) => {\r\n        console.log('completionResult: ', completionResult)\r\n        const timings = `${completionResult.timings.predicted_per_token_ms.toFixed()}ms per token, ${completionResult.timings.predicted_per_second.toFixed(\r\n          2,\r\n        )} tokens per second`\r\n        setMessages((msgs: Messages) => {\r\n          const index = msgs.findIndex((msg: Message) => msg.id === id)\r\n          if (index >= 0) {\r\n            return msgs.map((msg: Message, i: number) => {\r\n              if (msg.type === 'text' && i === index) {\r\n                return {\r\n                  ...msg,\r\n                  metadata: {\r\n                    ...msg.metadata,\r\n                    timings,\r\n                  },\r\n                }\r\n              }\r\n              return msg\r\n            })\r\n          }\r\n          return msgs\r\n        })\r\n        setInferencing(false)\r\n      })\r\n      .catch((err: LlamaError) => {\r\n        console.log('completion error: ', err)\r\n        setInferencing(false)\r\n        addSystemMessage(`Completion failed: ${err.message}`)\r\n      })\r\n  }\r\n\r\n  return (\r\n    <SafeAreaProvider>\r\n      <Chat\r\n        renderBubble={renderBubble}\r\n        theme={darkTheme}\r\n        messages={messages}\r\n        onSendPress={handleSendPress}\r\n        user={user}\r\n        onAttachmentPress={!context ? handlePickModel : undefined}\r\n        textInputProps={{\r\n          editable: !!context,\r\n          placeholder: !context\r\n            ? 'Press the file icon to pick a model'\r\n            : 'Type your message here',\r\n        }}\r\n      />\r\n    </SafeAreaProvider>\r\n  )\r\n}\r\n"
        }
    ]
}